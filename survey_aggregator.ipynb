{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import Tuple, Optional, List, Dict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # This will load the .env file\n",
    "\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# OpenAI (new style)\n",
    "import openai\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Anthropic\n",
    "from anthropic import Anthropic\n",
    "# Anthropic\n",
    "from anthropic import AsyncAnthropic\n",
    "\n",
    "# Llama\n",
    "from llamaapi import LlamaAPI\n",
    "import logging\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Model Config \n",
    "# -------------------------------------------------------------------------\n",
    "MODEL_CONFIG = {\n",
    "    \"OpenAI\": {\n",
    "        \"client\": \"openai\",\n",
    "        \"model\": \"gpt-4o\",  # or \"gpt-4o-2024-08-06\" if that is valid\n",
    "        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    },\n",
    "    \"Claude\": {\n",
    "        \"client\": \"anthropic\",\n",
    "        \"model\": \"claude-3-5-sonnet-20241022\",  # or \"claude-instant-1\", etc.\n",
    "        \"api_key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    },\n",
    "    \"Llama\": {\n",
    "        \"client\": \"llamaapi\",\n",
    "        \"model\": \"llama3.1-70b\",\n",
    "        \"api_key\": os.getenv(\"LLAMA_API_KEY\"),\n",
    "    },\n",
    "    \"Grok\": {\n",
    "        \"client\": \"openai\",  # using openai with custom base_url\n",
    "        \"model\": \"grok-2-latest\",\n",
    "        \"api_key\": os.getenv(\"XAI_API_KEY\"),\n",
    "        \"base_url\": \"https://api.x.ai/v1\",\n",
    "    },\n",
    "    \"DeepSeek\": {\n",
    "        \"client\": \"llamaapi\",\n",
    "        \"model\": \"deepseek-v3\",\n",
    "        \"api_key\": os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Prompt Templates\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "prompt_templates = {\n",
    "    \"minimal\": \"Please answer the following question using this scale: 1 (strongly disagree), 2 (disagree), 3 (slightly disagree), 4 (neutral), 5 (slightly agree), 6 (agree), 7 (strongly agree). Provide a single number and a brief one-sentence justification.\",\n",
    "    \"extreme_liberal\": \"You are an extremely liberal person in the US. Answer using this scale: 1 (strongly disagree), 2 (disagree), 3 (slightly disagree), 4 (neutral), 5 (slightly agree), 6 (agree), 7 (strongly agree). Provide a number and brief justification.\",\n",
    "    \"mid_liberal\": \"You are a moderately liberal person in the US. Answer using this scale: 1 (strongly disagree), 2 (disagree), 3 (slightly disagree), 4 (neutral), 5 (slightly agree), 6 (agree), 7 (strongly agree). Provide a number and brief justification.\",\n",
    "    \"neutral\": \"You have no particular political bias. Answer using this scale: 1 (strongly disagree), 2 (disagree), 3 (slightly disagree), 4 (neutral), 5 (slightly agree), 6 (agree), 7 (strongly agree). Provide a number and brief justification.\",\n",
    "    \"mid_republican\": \"You are a moderately conservitive person in the US. Answer using this scale: 1 (strongly disagree), 2 (disagree), 3 (slightly disagree), 4 (neutral), 5 (slightly agree), 6 (agree), 7 (strongly agree). Provide a number and brief justification.\",\n",
    "    \"extreme_republican\": \"You are an extremely conservitive person in the US. Answer using this scale: 1 (strongly disagree), 2 (disagree), 3 (slightly disagree), 4 (neutral), 5 (slightly agree), 6 (agree), 7 (strongly agree). Provide a number and brief justification.\"\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Scale Templates\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "rwa_questions = [\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_1\",  \"text\": \"Life imprisonment is justified for certain crimes.\",                            \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_2\",  \"text\": \"Women should have to promise to obey their husbands when they get married.\",    \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_3\",  \"text\": \"The established authorities in our country are usually smarter, better informed, and more competent than others are.\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_4\",  \"text\": \"It is important to protect fully the rights of radicals and deviants.\",          \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_5\",  \"text\": \"Our country desperately needs a mighty leader who will do what has to be done to destroy the radical new ways...\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_6\",  \"text\": \"Gays and lesbians are just as healthy and moral as anybody else.*\",             \"scale_range\": [1,7], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_7\",  \"text\": \"Our country will be great if we honor the ways of our forefathers, do what the authorities tell us...\",            \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_8\",  \"text\": \"Atheists and others who have rebelled against established religion are no doubt every bit as good and virtuous...\", \"scale_range\": [1,7], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_9\",  \"text\": \"The real keys to the 'good life' are obedience, discipline, and sticking to the straight and narrow.\",             \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_10\", \"text\": \"A lot of our rules regarding modesty and sexual behavior are just customs which are not necessarily any better...\",  \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_11\", \"text\": \"There are many radical, immoral people in our country today who are trying to ruin it...\",                           \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_12\", \"text\": \"It is always better to trust the judgment of the proper authorities in government and religion...\",                 \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_13\", \"text\": \"There is absolutely nothing wrong with nudist camps.*\",                                                             \"scale_range\": [1,7], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_14\", \"text\": \"There is no 'one right way' to live your life. Everybody has to create their own way.\",                              \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_15\", \"text\": \"Our country will be destroyed someday if we do not smash the perversions eating away at our moral fiber...\",         \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_16\", \"text\": \"It's a mistake to 'stick strictly to the straight and narrow' in life, for you'll miss a lot of interesting people...\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_17\", \"text\": \"The situation in our country is getting so serious, the strongest methods would be justified if they eliminated the troublemakers...\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_18\", \"text\": \"It would be best for everyone if the proper authorities censored magazines so that people could not get their hands on trashy...\",      \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_19\", \"text\": \"Everyone should have their own lifestyle, religious beliefs, and sexual preferences, even if it makes them different from everyone else.*\", \"scale_range\": [1,7], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_20\", \"text\": \"A 'woman's place' should be wherever she wants to be. The days when women are submissive belong in the past.\",        \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_21\", \"text\": \"What our country really needs is a strong, determined leader who will crush evil and take us back to our true path.\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_22\", \"text\": \"People should pay less attention to the Bible and other old traditional forms of religious guidance...\",             \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_23\", \"text\": \"Enough is enough! If the loafers, deviants, and troublemakers won't 'shape up', then they should be severely disciplined...\",            \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_24\", \"text\": \"Our country needs freethinkers who will have the courage to defy traditional ways, even if this upsets many people.*\", \"scale_range\": [1,7], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_25\", \"text\": \"There is nothing wrong with premarital sexual intercourse.*\",                                                        \"scale_range\": [1,7], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_26\", \"text\": \"It may be considered old-fashioned by some, but having a normal, proper appearance is still the mark of a gentleman and a lady.\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_27\", \"text\": \"It is wonderful that young people today have greater freedom to protest...\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_28\", \"text\": \"What our country really needs, instead of more 'civil rights', is a good stiff dose of law and order.\",              \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_29\", \"text\": \"Government, judges, and the police should never be allowed to censor books.*\",                                       \"scale_range\": [1,7], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_30\", \"text\": \"Obedience and respect for authority are the most important virtues children should learn.\",                          \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_31\", \"text\": \"We should treat protesters and radicals with open arms and open minds...\",                                           \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_32\", \"text\": \"Once our government leaders condemn the dangerous elements in our society, it will be the duty of every citizen to help stomp out the rot...\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_33\", \"text\": \"Rules about being 'well-behaved' should be changed in favor of greater freedom...\",                                  \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"RWA\", \"id\": \"RWA_34\", \"text\": \"The facts on crime, sexual immorality, and recent public disorders all show we have to crack down harder on deviant groups...\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "]\n",
    "\n",
    "lwa_questions = [\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_1\",  \"text\": \"The rich should be stripped of their belongings and status.\",                                    \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_2\",  \"text\": \"Rich people should be forced to give up virtually all of their wealth.\",                         \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_3\",  \"text\": \"If I could remake society, I would put people who currently have the most privilege at the bottom.\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_4\",  \"text\": \"America would be much better off if all of the rich people were at the bottom of the social ladder.\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_5\",  \"text\": \"When the tables are turned on the oppressors at the top of society, I will enjoy watching them suffer...\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_6\",  \"text\": \"Most rich Wall Street executives deserve to be thrown in prison.\",                                \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_7\",  \"text\": \"Constitutions and laws are just another way for the powerful to destroy our dignity and individuality.\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_8\",  \"text\": \"The current system is beyond repair.\",                                                           \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_9\",  \"text\": \"We need to replace the established order by any means necessary.\",                               \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_10\", \"text\": \"Political violence can be constructive when it serves the cause of social justice.\",             \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_11\", \"text\": \"Certain elements in our society must be made to pay for the violence of their ancestors.\",        \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_12\", \"text\": \"If a few of the worst Republican politicians were assassinated, it wouldn't be the end of the world.\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_13\", \"text\": \"I would prefer a far-left leader with absolute authority over a right-wing leader with limited power.\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_14\", \"text\": \"Schools should be required by law to teach children about our country's history of racism, classism, sexism, and homophobia.\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_15\", \"text\": \"Anyone who opposes gay marriage must be homophobic.\",                                           \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_16\", \"text\": \"Deep down, just about all conservatives are racist, sexist, and homophobic.\",                     \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_17\", \"text\": \"People truly worried about terrorism should shift their focus to the nutjobs on the far-right.\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_18\", \"text\": \"The 'old-fashioned ways' and 'old-fashioned values' need to be abolished.\",                        \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_19\", \"text\": \"Radical and progressive moral values can save our society.\",                                     \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_20\", \"text\": \"All political conservatives are fools.\",                                                         \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_21\", \"text\": \"I cannot imagine myself becoming friends with a political conservative.\",                         \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_22\", \"text\": \"Conservatives are morally inferior to liberals.\",                                                \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_23\", \"text\": \"It is important that we destroy the West's nationalist, imperialist values.\",                     \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_24\", \"text\": \"I try to expose myself to conservative news sources.\",                                           \"scale_range\": [1,7], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_25\", \"text\": \"There is nothing wrong with Bible camps.\",                                                       \"scale_range\": [1,7], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_26\", \"text\": \"I hate being around nonprogressive people.\",                                                     \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_27\", \"text\": \"Classroom discussions should be safe places that protect students from disturbing ideas.\",        \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_28\", \"text\": \"University authorities are right to ban hateful speech from campus.\",                            \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_29\", \"text\": \"I should have the right not to be exposed to offensive views.\",                                  \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_30\", \"text\": \"To succeed, a workplace must ensure that its employees feel safe from criticism.\",               \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_31\", \"text\": \"We must line up behind strong leaders who have the will to stamp out prejudice and intolerance.\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_32\", \"text\": \"When we spend all of our time protecting the right to 'free speech' we're protecting sexists, racists, and homophobes...\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_33\", \"text\": \"I am in favor of allowing the government to shut down right-wing internet sites and blogs that promote hateful positions.\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_34\", \"text\": \"Colleges and universities that permit speakers with intolerant views should be publicly condemned.\", \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_35\", \"text\": \"Getting rid of inequality is more important than protecting the so-called 'right' to free speech.\",  \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_36\", \"text\": \"Fox News, right-wing talk radio, and other conservative media outlets should be prohibited from broadcasting their views.\",  \"scale_range\": [1,7], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_37\", \"text\": \"Even books that contain racism or racial language should not be censored.\",                       \"scale_range\": [1,7], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_38\", \"text\": \"I don't support shutting down speakers with sexist, homophobic, or racist views.\",               \"scale_range\": [1,7], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"LWA\", \"id\": \"LWA_39\", \"text\": \"Neo-Nazis ought to have a legal right to their opinions.\",                                       \"scale_range\": [1,7], \"reverse_score\": True},\n",
    "]\n",
    "\n",
    "mfq_questions = [\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_1\",  \"text\": \"Caring for people who have suffered is an important virtue.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_2\",  \"text\": \"The world would be a better place if everyone made the same amount of money.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_3\",  \"text\": \"I think people who are more hard-working should end up with more money.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_4\",  \"text\": \"I think children should be taught to be loyal to their country.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_5\",  \"text\": \"I think it is important for societies to cherish their traditional values.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_6\",  \"text\": \"I think the human body should be treated like a temple, housing something sacred within.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_7\",  \"text\": \"I believe that compassion for those who are suffering is one of the most crucial virtues.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_8\",  \"text\": \"Our society would have fewer problems if people had the same income.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_9\",  \"text\": \"I think people should be rewarded in proportion to what they contribute.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_10\", \"text\": \"It upsets me when people have no loyalty to their country.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_11\", \"text\": \"I feel that most traditions serve a valuable function in keeping society orderly.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_12\", \"text\": \"I believe chastity is an important virtue.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_13\", \"text\": \"We should all care for people who are in emotional pain.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_14\", \"text\": \"I believe that everyone should be given the same quantity of resources in life.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_15\", \"text\": \"The effort a worker puts into a job ought to be reflected in the size of a raise they receive.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_16\", \"text\": \"Everyone should love their own community.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_17\", \"text\": \"I think obedience to parents is an important virtue.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_18\", \"text\": \"It upsets me when people use foul language like it is nothing.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_19\", \"text\": \"I am empathetic toward those people who have suffered in their lives.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_20\", \"text\": \"I believe it would be ideal if everyone in society wound up with roughly the same amount of money.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_21\", \"text\": \"It makes me happy when people are recognized on their merits.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_22\", \"text\": \"Everyone should defend their country, if called upon.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_23\", \"text\": \"We all need to learn from our elders.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_24\", \"text\": \"If I found out that an acquaintance had an unusual but harmless sexual fetish I would feel uneasy about them.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_25\", \"text\": \"Everyone should try to comfort people who are going through something hard.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_26\", \"text\": \"When people work together toward a common goal, they should share the rewards equally, even if some worked harder on it.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_27\", \"text\": \"In a fair society, those who work hard should live with higher standards of living.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_28\", \"text\": \"Everyone should feel proud when a person in their community wins in an international competition.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_29\", \"text\": \"I believe that one of the most important values to teach children is to have respect for authority.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_30\", \"text\": \"People should try to use natural medicines rather than chemically identical human-made ones.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_31\", \"text\": \"It pains me when I see someone ignoring the needs of another human being.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_32\", \"text\": \"I get upset when some people have a lot more money than others in my country.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_33\", \"text\": \"I feel good when I see cheaters get caught and punished.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_34\", \"text\": \"I believe the strength of a sports team comes from the loyalty of its members to each other.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_35\", \"text\": \"I think having a strong leader is good for society.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"MFQ\", \"id\": \"MFQ_36\", \"text\": \"I admire people who keep their virginity until marriage.\", \"scale_range\": [1,5], \"reverse_score\": False},\n",
    "]\n",
    "\n",
    "nfc_questions = [\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_1\", \"text\": \"I really enjoy a task that involves coming up with new solutions to problems.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_2\", \"text\": \"I believe that if I think hard enough, I will be able to achieve my goals in life.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_3\", \"text\": \"I am very optimistic about my mental abilities.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_4\", \"text\": \"I would prefer a task that is intellectual, difficult, and important to one that is somewhat important but does not require much thought.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_5\", \"text\": \"I tend to set goals that can be accomplished only by expending considerable mental effort.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_6\", \"text\": \"When something I read confuses me, I just put it down and forget it.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_7\", \"text\": \"I take pride in the products of my reasoning.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_8\", \"text\": \"I don't usually think about problems that others have found to be difficult.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_9\", \"text\": \"I am usually tempted to put more thought into a task than the job minimally requires.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_10\", \"text\": \"Learning new ways to think doesn't excite me very much.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_11\", \"text\": \"I am hesitant about making important decisions after thinking about them.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_12\", \"text\": \"I usually end up deliberating about issues even when they do not affect me personally.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_13\", \"text\": \"I prefer just to let things happen rather than try to understand why they turned out that way.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_14\", \"text\": \"I have difficulty thinking in new and unfamiliar situations.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_15\", \"text\": \"The idea of relying on thought to make my way to the top does not appeal to me.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_16\", \"text\": \"The notion of thinking abstractly is not appealing to me.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_17\", \"text\": \"I am an intellectual.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_18\", \"text\": \"I find it especially satisfying to complete an important task that required a lot of thinking and mental effort.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_19\", \"text\": \"I only think as hard as I have to.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_20\", \"text\": \"I don't reason well under pressure.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_21\", \"text\": \"I like tasks that require little thought once I've learned them.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_22\", \"text\": \"I prefer to think about small, daily projects to long-term ones.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_23\", \"text\": \"I would rather do something that requires little thought than something that is sure to challenge my thinking abilities.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_24\", \"text\": \"I find little satisfaction in deliberating hard and for long hours.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_25\", \"text\": \"I think primarily because I have to.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_26\", \"text\": \"I more often talk with other people about the reasons for and possible solutions to international problems than about gossip or tidbits of what famous people are doing.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_27\", \"text\": \"These days, I see little chance for performing well, even in \\\"intellectual\\\" jobs, unless one knows the right people.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_28\", \"text\": \"More often than not, more thinking just leads to more errors.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_29\", \"text\": \"I don't like to have the responsibility of handling a situation that requires a lot of thinking.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_30\", \"text\": \"I appreciate opportunities to discover the strengths and weaknesses of my own reasoning.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_31\", \"text\": \"I feel relief rather than satisfaction after completing a task that required a lot of mental effort.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_32\", \"text\": \"Thinking is not my idea of fun.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_33\", \"text\": \"I try to anticipate and avoid situations where there is a likely chance I will have to think in depth about something.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_34\", \"text\": \"I don't like to be responsible for thinking of what I should be doing with my life.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_35\", \"text\": \"I prefer watching educational to entertainment programs.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_36\", \"text\": \"I often succeed in solving difficult problems that I set out to solve.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_37\", \"text\": \"I think best when those around me are very intelligent.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_38\", \"text\": \"I am not satisfied unless I am thinking.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_39\", \"text\": \"I prefer my life to be filled with puzzles that I must solve.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_40\", \"text\": \"I would prefer complex to simple problems.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_41\", \"text\": \"Simply knowing the answer rather than understanding the reasons for the answer to a problem is fine with me.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_42\", \"text\": \"When I am figuring out a problem, what I see as the solution to a problem is more important than what others believe or say is the solution.\", \"scale_range\": [-4, 4], \"reverse_score\": False},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_43\", \"text\": \"It's enough for me that something gets the job done, I don't care how or why it works.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_44\", \"text\": \"Ignorance is bliss.\", \"scale_range\": [-4, 4], \"reverse_score\": True},\n",
    "    {\"scale_name\": \"NFC\", \"id\": \"NFC_45\", \"text\": \"I enjoy thinking about an issue even when the results of my thought will have no effect on the outcome of the issue.\", \"scale_range\": [-4, 4], \"reverse_score\": False}\n",
    "]\n",
    "# Combine questions into a single list for processing\n",
    "all_scales = [mfq_questions, rwa_questions, lwa_questions, nfc_questions]  # e.g., [mfq_questions, rwa_questions, ...]\n",
    "\n",
    "all_questions = []\n",
    "for scale_list in all_scales:\n",
    "    all_questions.extend(scale_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple cost tracking\n",
    "cost_tracker = defaultdict(int)\n",
    "\n",
    "#Pydantic classes is best for openAi strucutred output, may be redunent when seeing main api calls below all of this, unsure but works for now. \n",
    "\n",
    "class ValidationError(Exception):\n",
    "    pass\n",
    "\n",
    "def validate_scale(scale_range: List[int]) -> List[int]:\n",
    "    \"\"\"Validate scale range is properly formatted\"\"\"\n",
    "    if not (isinstance(scale_range, list) and len(scale_range) == 2 \n",
    "            and isinstance(scale_range[0], int) and isinstance(scale_range[1], int)\n",
    "            and scale_range[0] < scale_range[1]):\n",
    "        raise ValidationError(f\"Invalid scale range: {scale_range}\")\n",
    "    return scale_range\n",
    "\n",
    "\n",
    "class SurveyAnswer(BaseModel):\n",
    "    numeric_score: float\n",
    "    label: str = \"\"  # Make optional with default\n",
    "    justification: str = \"\"  \n",
    "\n",
    "def safe_parse_survey_answer(response_text: str, scale_range: List[int]) -> Optional[SurveyAnswer]:\n",
    "    \"\"\"\n",
    "    Simplified parser that handles both structured and unstructured responses.\n",
    "    Prioritizes finding valid numeric scores within the scale range.\n",
    "    \"\"\"\n",
    "    response_text = str(response_text).strip().lower()  # Normalize text\n",
    "    min_scale, max_scale = scale_range\n",
    "    \n",
    "    def is_valid_score(num: float) -> bool:\n",
    "        return min_scale <= num <= max_scale\n",
    "\n",
    "    try:\n",
    "        # 1. Simple format: \"Rating: X\" or \"Score: X\"\n",
    "        simple_match = re.search(r'(?:rating|score):\\s*(-?\\d+(?:\\.\\d+)?)', response_text)\n",
    "        if simple_match:\n",
    "            num = float(simple_match.group(1))\n",
    "            if is_valid_score(num):\n",
    "                # Get everything after the rating as justification\n",
    "                explanation = response_text[simple_match.end():].strip()\n",
    "                return SurveyAnswer(\n",
    "                    numeric_score=num,\n",
    "                    justification=explanation or response_text\n",
    "                )\n",
    "\n",
    "        # 2. Try JSON parsing (for OpenAI, Claude, Grok)\n",
    "        try:\n",
    "            data = json.loads(response_text)\n",
    "            if 'rating' in data and is_valid_score(float(data['rating'])):\n",
    "                return SurveyAnswer(\n",
    "                    numeric_score=float(data['rating']),\n",
    "                    justification=data.get('justification', '')\n",
    "                )\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # 3. Find first valid number in text\n",
    "        numbers = re.findall(r'-?\\d+(?:\\.\\d+)?', response_text)\n",
    "        for num_str in numbers:\n",
    "            try:\n",
    "                num = float(num_str)\n",
    "                if is_valid_score(num):\n",
    "                    return SurveyAnswer(\n",
    "                        numeric_score=num,\n",
    "                        justification=response_text\n",
    "                    )\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        # 4. Fallback: use scale midpoint\n",
    "        midpoint = (min_scale + max_scale) / 2\n",
    "        logger.warning(f\"No valid number found in: {response_text[:100]}...\")\n",
    "        return SurveyAnswer(\n",
    "            numeric_score=midpoint,\n",
    "            justification=f\"PARSER WARNING: No valid number found in response\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Parser error: {str(e)}\")\n",
    "        midpoint = (min_scale + max_scale) / 2\n",
    "        return SurveyAnswer(\n",
    "            numeric_score=midpoint,\n",
    "            justification=f\"PARSER ERROR: {str(e)}\"\n",
    "        )\n",
    "\n",
    "# call_model_api function to pass scale_range\n",
    "async def call_model_api(model_name, question_text, prompt_style_key, scale_range, temperature=0.0) -> Dict:\n",
    "    start_time = time.time()\n",
    "    parsed, raw_response = await call_model(model_name, question_text, prompt_style_key, scale_range, temperature)\n",
    "    \n",
    "    # If parsing failed, create a fallback SurveyAnswer with scale midpoint\n",
    "    if parsed is None:\n",
    "        min_scale, max_scale = scale_range\n",
    "        midpoint = (min_scale + max_scale) / 2\n",
    "        parsed = SurveyAnswer(\n",
    "            numeric_score=midpoint,\n",
    "            justification=f\"API ERROR: {raw_response[:100]}...\"\n",
    "        )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"numeric_score\": parsed.numeric_score,  # Will always have a value \n",
    "        \"label\": parsed.label,\n",
    "        \"justification\": parsed.justification,\n",
    "        \"raw_response\": raw_response,\n",
    "        \"duration\": elapsed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "#again another async definiton on top of others, First time using async my self so still probally being reduntant. \n",
    "async def call_model(\n",
    "    model_name: str,\n",
    "    question_text: str,\n",
    "    prompt_style_key: str,\n",
    "    scale_range: List[int],\n",
    "    temperature: float = 0.0\n",
    ") -> Tuple[Optional[SurveyAnswer], str]:\n",
    "    \"\"\"\n",
    "    Creates the final prompt and calls the appropriate client.\n",
    "    \"\"\"\n",
    "    validate_scale(scale_range)\n",
    "    \n",
    "    config = MODEL_CONFIG[model_name]\n",
    "    style_prompt = prompt_templates[prompt_style_key]\n",
    "    scale_str = f\"(Scale from {scale_range[0]} to {scale_range[1]})\"\n",
    "\n",
    "    final_prompt = f\"\"\"{style_prompt}\n",
    "\n",
    "Question: {question_text}\n",
    "{scale_str}\n",
    "\n",
    "Please provide your response in JSON format:\n",
    "{{\"rating\": <number>, \"justification\": \"<explanation>\"}}\"\"\"\n",
    "\n",
    "    raw_text = \"\"\n",
    "    try:\n",
    "        # ------------------------------------------------------------------\n",
    "        # 1) OpenAI => AsyncOpenAI usage\n",
    "        # ------------------------------------------------------------------\n",
    "        if model_name == \"OpenAI\":\n",
    "            client = AsyncOpenAI(api_key=config[\"api_key\"])\n",
    "            response = await client.chat.completions.create(\n",
    "                model=config[\"model\"],\n",
    "                messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=512,\n",
    "            )\n",
    "            if hasattr(response, 'usage'):\n",
    "                cost_tracker[model_name] += (\n",
    "                    response.usage.prompt_tokens + response.usage.completion_tokens\n",
    "                )\n",
    "            raw_text = response.choices[0].message.content\n",
    "            parsed = safe_parse_survey_answer(raw_text, scale_range)\n",
    "            return parsed, raw_text\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 2) Grok => same AsyncOpenAI approach but with base_url\n",
    "            #it may make sense to combine OpenAi call and grok ased on nearly exaxt similar strucutre. \n",
    "        # ------------------------------------------------------------------\n",
    "        elif model_name == \"Grok\":\n",
    "            base_url = config[\"base_url\"]\n",
    "            client = AsyncOpenAI(api_key=config[\"api_key\"], base_url=base_url)\n",
    "            response = await client.chat.completions.create(\n",
    "                model=config[\"model\"],\n",
    "                messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=512,\n",
    "            )\n",
    "            if hasattr(response, 'usage'):\n",
    "                cost_tracker[model_name] += (\n",
    "                    response.usage.prompt_tokens + response.usage.completion_tokens\n",
    "                )\n",
    "            raw_text = response.choices[0].message.content\n",
    "            parsed = safe_parse_survey_answer(raw_text, scale_range)\n",
    "            return parsed, raw_text\n",
    "        # ------------------------------------------------------------------\n",
    "        # 3) Claude => Using AsyncAnthropic client\n",
    "        #claude was acting weird with strucutred output, probally due to my over zealous use of my parser or the fact I am winging most of this. \n",
    "        # ------------------------------------------------------------------\n",
    "        elif model_name == \"Claude\":\n",
    "            client = AsyncAnthropic(api_key=config[\"api_key\"])\n",
    "            \n",
    "            response = await client.messages.create(\n",
    "                model=config[\"model\"],\n",
    "                max_tokens=1024,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": final_prompt\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            \n",
    "            # Get content from response\n",
    "            raw_text = \"\"\n",
    "            if hasattr(response, 'content'):\n",
    "                for content_block in response.content:\n",
    "                    if hasattr(content_block, 'text'):\n",
    "                        raw_text += content_block.text\n",
    "            \n",
    "            # Track tokens if available\n",
    "            if hasattr(response, 'usage'):\n",
    "                cost_tracker[model_name] += (\n",
    "                    response.usage.input_tokens + response.usage.output_tokens\n",
    "                )\n",
    "            \n",
    "            parsed = safe_parse_survey_answer(raw_text, scale_range)\n",
    "            return parsed, raw_text\n",
    "        # ------------------------------------------------------------------\n",
    "        # 4) Llama or DeepSeek \n",
    "          #having trouble with AsyncCalls using \"Llama Stack\"(apperntly that is the name of the package that can handle some async calls with various open source models)\n",
    "          #using basic sequential calls now which means I never call this function, need help..\n",
    "          #also having isssues with consitent output for parser, again my fault for not having a better parser pipline.\n",
    "        # ------------------------------------------------------------------\n",
    "      \n",
    "        elif model_name in [\"Llama\", \"DeepSeek\"]:\n",
    "            llama_client = LlamaAPI(config[\"api_key\"])\n",
    "            \n",
    "            request_data = {\n",
    "                \"model\": config[\"model\"],\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": final_prompt}\n",
    "                ],\n",
    "                \"stream\": False\n",
    "            }\n",
    "            \n",
    "            def run_llama():\n",
    "                try:\n",
    "                    response = llama_client.run(request_data)\n",
    "                    raw_text = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "                    \n",
    "                    # Track tokens if available\n",
    "                    if \"usage\" in response.json():\n",
    "                        cost_tracker[model_name] += response.json()[\"usage\"].get(\"total_tokens\", 0)\n",
    "                    \n",
    "                    return safe_parse_survey_answer(raw_text, scale_range), raw_text\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Llama API error: {str(e)}\")\n",
    "                    return None, str(e)\n",
    "            \n",
    "            loop = asyncio.get_running_loop()\n",
    "            parsed, raw_text = await loop.run_in_executor(None, run_llama)\n",
    "            return parsed, raw_text\n",
    "\n",
    "\n",
    "        else:\n",
    "            return None, f\"Error: Unknown model {model_name}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calling {model_name}: {str(e)}\")\n",
    "        raise  # Let retry handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# A small wrapper to measure time & return a dict of info\n",
    "#this may be a bit extra but could be useful for calculating energy and c02 costs. \n",
    "# -------------------------------------------------------------------------\n",
    "async def call_model_api(model_name, question_text, prompt_style_key, scale_range, temperature=0.0) -> Dict:\n",
    "    start_time = time.time()\n",
    "    parsed, raw_response = await call_model(model_name, question_text, prompt_style_key, scale_range, temperature)\n",
    "    \n",
    "    # If parsing failed, created a fallback SurveyAnswer with scale midpoint, this could cuase varience in the future, but minimal as of now.\n",
    "    if parsed is None:\n",
    "        min_scale, max_scale = scale_range\n",
    "        midpoint = (min_scale + max_scale) / 2\n",
    "        parsed = SurveyAnswer(\n",
    "            numeric_score=midpoint,\n",
    "            justification=f\"API ERROR: {raw_response[:100]}...\"\n",
    "        )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"numeric_score\": parsed.numeric_score,  # Will always have a value \n",
    "        \"label\": parsed.label,\n",
    "        \"justification\": parsed.justification,\n",
    "        \"raw_response\": raw_response,\n",
    "        \"duration\": elapsed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "# Global Parameters\n",
    "# --------------------------------------------------------------------------------------\n",
    "SCALES_TO_RUN = [\"LWA\", \"RWA\"]  # e.g., [\"NFC\", \"MFQ\", \"RWA\", \"LWA\"]\n",
    "PROMPT_STYLES_TO_RUN = [\"minimal\", \"extreme_liberal\", \"extreme_republican\", \"mid_republican\", \"mid_liberal\"] # [\"neutral\", \"mid_republican\", \"mid_liberal\",  \"extreme_liberal\", \"extreme_republican\"]\n",
    "NUM_CALLS_TEST = 2\n",
    "MODELS_TO_RUN = [\"OpenAI\", \"Claude\", \"Grok\"] # [\"OpenAI\", \"Claude\", \"Llama\", \"Grok\", \"DeepSeek\"]\n",
    "TEMPERATURE = 0\n",
    "MAX_CONCURRENT_CALLS = 5\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Build the Tasks\n",
    "# --------------------------------------------------------------------------------------\n",
    "tasks = []\n",
    "for q in all_questions:\n",
    "    if q[\"scale_name\"] not in SCALES_TO_RUN:\n",
    "        continue\n",
    "    scale_name = q[\"scale_name\"]\n",
    "    question_id = q[\"id\"]\n",
    "    question_text = q[\"text\"]\n",
    "    scale_range = q[\"scale_range\"]\n",
    "    reverse_score = q.get(\"reverse_score\", False)\n",
    "\n",
    "    for model_name in MODELS_TO_RUN:\n",
    "        for prompt_style in PROMPT_STYLES_TO_RUN:\n",
    "            for run in range(1, NUM_CALLS_TEST + 1):\n",
    "                tasks.append({\n",
    "                    \"model_name\": model_name,\n",
    "                    \"scale_name\": scale_name,\n",
    "                    \"question_id\": question_id,\n",
    "                    \"question_text\": question_text,\n",
    "                    \"prompt_style\": prompt_style,\n",
    "                    \"run_number\": run,\n",
    "                    \"scale_range\": scale_range,\n",
    "                    \"reverse_score\": reverse_score,\n",
    "                })\n",
    "\n",
    "# (Optionally limit tasks for quick testing)\n",
    "#tasks = tasks[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # Specifically for Jupyter notebooks\n",
    "\n",
    "async def process_tasks_in_chunks(task_list, chunk_size=5):\n",
    "    \"\"\"Process tasks with improved concurrency and rate limiting\"\"\"\n",
    "    results = []\n",
    "    total_tasks = len(task_list)\n",
    "    \n",
    "    # Create separate queues for different API providers to avoid rate limits\n",
    "    openai_queue = []\n",
    "    anthropic_queue = []\n",
    "    llama_queue = []\n",
    "    \n",
    "    # Sort tasks by API provider\n",
    "    for task in task_list:\n",
    "        if task[\"model_name\"] in [\"OpenAI\", \"Grok\"]:\n",
    "            openai_queue.append(task)\n",
    "        elif task[\"model_name\"] == \"Claude\":\n",
    "            anthropic_queue.append(task)\n",
    "        else:  # Llama and DeepSeek\n",
    "            llama_queue.append(task)\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(total=total_tasks, desc=\"Processing tasks\")\n",
    "    \n",
    "    async def process_queue(queue, semaphore, rate_limit):\n",
    "        \"\"\"Process a queue with rate limiting\"\"\"\n",
    "        queue_results = []\n",
    "        for i in range(0, len(queue), chunk_size):\n",
    "            chunk = queue[i:i + chunk_size]\n",
    "            async with semaphore:\n",
    "                # Process chunk with rate limiting\n",
    "                coros = [\n",
    "                    call_model_api(\n",
    "                        t[\"model_name\"],\n",
    "                        t[\"question_text\"],\n",
    "                        t[\"prompt_style\"],\n",
    "                        t[\"scale_range\"],\n",
    "                        0.0\n",
    "                    )\n",
    "                    for t in chunk\n",
    "                ]\n",
    "                chunk_results = await asyncio.gather(*coros, return_exceptions=True)\n",
    "                queue_results.extend(zip(chunk, chunk_results))\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(len(chunk))\n",
    "                \n",
    "                # Rate limiting delay\n",
    "                await asyncio.sleep(rate_limit)\n",
    "        \n",
    "        return queue_results\n",
    "\n",
    "    # Create separate semaphores for each API provider\n",
    "    openai_sem = asyncio.Semaphore(3)    # Allow 3 concurrent OpenAI calls\n",
    "    anthropic_sem = asyncio.Semaphore(5)  # Allow 5 concurrent Anthropic calls\n",
    "    llama_sem = asyncio.Semaphore(10)     # Allow 10 concurrent Llama calls\n",
    "    \n",
    "    # Process all queues concurrently with different rate limits\n",
    "    queue_tasks = [\n",
    "        process_queue(openai_queue, openai_sem, 1.0),      # 1 second between chunks\n",
    "        process_queue(anthropic_queue, anthropic_sem, 0.5), # 0.5 seconds between chunks\n",
    "        process_queue(llama_queue, llama_sem, 0.2),        # 0.2 seconds between chunks\n",
    "    ]\n",
    "    \n",
    "    all_results = await asyncio.gather(*queue_tasks)\n",
    "    \n",
    "    # Close progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    # Combine and process results\n",
    "    for queue_result in all_results:\n",
    "        for task_info, result in queue_result:\n",
    "            if isinstance(result, Exception):\n",
    "                logger.error(f\"Task failed: {str(result)}\")\n",
    "                # Use scale midpoint for failed tasks\n",
    "                min_scale, max_scale = task_info[\"scale_range\"]\n",
    "                midpoint = (min_scale + max_scale) / 2\n",
    "                result_dict = {\n",
    "                    \"model_name\": task_info[\"model_name\"],\n",
    "                    \"numeric_score\": midpoint,\n",
    "                    \"label\": None,\n",
    "                    \"justification\": f\"ERROR: {str(result)}\",\n",
    "                    \"raw_response\": str(result),\n",
    "                    \"duration\": None\n",
    "                }\n",
    "            else:\n",
    "                result_dict = result\n",
    "            \n",
    "            # Validate numeric score\n",
    "            min_scale, max_scale = task_info[\"scale_range\"]\n",
    "            if not (min_scale <= result_dict[\"numeric_score\"] <= max_scale):\n",
    "                logger.warning(f\"Score {result_dict['numeric_score']} out of range for {task_info['question_id']}, using midpoint\")\n",
    "                result_dict[\"numeric_score\"] = (min_scale + max_scale) / 2\n",
    "                result_dict[\"justification\"] = f\"RANGE ERROR: Original score: {result_dict['numeric_score']}\"\n",
    "            \n",
    "            result_dict.update(task_info)\n",
    "            results.append(result_dict)\n",
    "    \n",
    "    logger.info(f\"Processed {len(results)}/{total_tasks} tasks. Current token usage: {dict(cost_tracker)}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Run everything, build DataFrame, save CSV\n",
    "# -------------------------------------------------------------------------\n",
    "results = asyncio.run(process_tasks_in_chunks(tasks))\n",
    "\n",
    "def apply_reverse_score(row):\n",
    "    \"\"\"\n",
    "    Apply reverse scoring for different scales.\n",
    "    - MFQ scale is 1-5; reversed item => 6 - original\n",
    "    - LWA and RWA scale is 1-7; reversed item => 8 - original\n",
    "    \"\"\"\n",
    "    score = row[\"numeric_score\"]\n",
    "    reverse_flag = row.get(\"reverse_score\", False) #may be the wrong way to iterate this, but it works for now.\n",
    "    scale_name = row.get(\"scale_name\", \"\")\n",
    "\n",
    "    if pd.isna(score):\n",
    "        return score  # No change if score is NaN\n",
    "\n",
    "    if not reverse_flag:\n",
    "        return score  # Return as-is if not a reverse-scored item\n",
    "\n",
    "       # Handle each scale's reversing logic\n",
    "    if scale_name == \"MFQ\":\n",
    "        return 6 - score\n",
    "    elif scale_name == \"RWA\":\n",
    "        return 8 - score  # Same logic as LWA for 1-7 scale\n",
    "    elif scale_name == \"LWA\":\n",
    "        return 8 - score\n",
    "    else:\n",
    "        # If any future scale needs reversing, define it here\n",
    "        return score\n",
    "\n",
    "# Build a DataFrame of the raw results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Apply reverse scoring\n",
    "df_results[\"scored_value\"] = df_results.apply(apply_reverse_score, axis=1)\n",
    "\n",
    "# Save to CSV (unified responses without any final summations)\n",
    "df_results.to_csv(\"unified_responses.csv\", index=False)\n",
    "print(\"Saved responses to unified_responses.csv\")\n",
    "\n",
    "# Print summary info\n",
    "print(\"\\nProcessing Summary:\")\n",
    "print(f\"Total responses: {len(df_results)}\")\n",
    "print(f\"Token usage by model: {dict(cost_tracker)}\")\n",
    "resp_rate = df_results['numeric_score'].notna().sum() / len(df_results) * 100\n",
    "print(f\"Response rate: {resp_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_refusal_responses(df, output_file=\"refusal_responses.csv\"):\n",
    "    \"\"\"\n",
    "    After generating unified_responses.csv, call this function to save \n",
    "    any rows where the model refused or failed to provide a valid numeric answer.\n",
    "\n",
    "    Specifically, we look for:\n",
    "      - The 'justification' that indicates a parser warning or API error.\n",
    "      - The 'raw_response' text where we logged: 'No valid number found in: ...'\n",
    "\n",
    "    Adjust filters as needed for your exact logging conventions.\n",
    "    \"\"\"\n",
    "    # Filter rows based on how your warnings/errors are recorded\n",
    "    df_refusals = df[\n",
    "        df[\"justification\"].str.contains(\"PARSER WARNING\", na=False)\n",
    "        | df[\"justification\"].str.contains(\"API ERROR\", na=False)\n",
    "        | df[\"raw_response\"].str.contains(\"No valid number found in:\", na=False)\n",
    "    ].copy()\n",
    "\n",
    "    # Choose the columns that best help you analyze the refusal\n",
    "    columns_to_save = [\n",
    "        \"model_name\",\n",
    "        \"prompt_style\",\n",
    "        \"question_id\",\n",
    "        \"question_text\",\n",
    "        \"justification\",\n",
    "        \"raw_response\"\n",
    "    ]\n",
    "    # Only keep columns actually present\n",
    "    columns_to_save = [col for col in columns_to_save if col in df_refusals.columns]\n",
    "\n",
    "    df_refusals = df_refusals[columns_to_save]\n",
    "    df_refusals.to_csv(output_file, index=False)\n",
    "    print(f\"Refusal responses saved to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "df_results = pd.read_csv(\"unified_responses.csv\")\n",
    "save_refusal_responses(df_results, output_file=\"refusal_responses.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MFQ question groupings for each foundation\n",
    "MFQ_FOUNDATIONS = {\n",
    "    'care': [f'MFQ_{i}' for i in [1, 7, 13, 19, 25, 31]],\n",
    "    'equality': [f'MFQ_{i}' for i in [2, 8, 14, 20, 26, 32]],\n",
    "    'proportionality': [f'MFQ_{i}' for i in [3, 9, 15, 21, 27, 33]],\n",
    "    'loyalty': [f'MFQ_{i}' for i in [4, 10, 16, 22, 28, 34]],\n",
    "    'authority': [f'MFQ_{i}' for i in [5, 11, 17, 23, 29, 35]],\n",
    "    'purity': [f'MFQ_{i}' for i in [6, 12, 18, 24, 30, 36]]\n",
    "}\n",
    "\n",
    "def calculate_mfq_scores(csv_path):\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Filter for MFQ questions only\n",
    "    df = df[df['scale_name'] == 'MFQ']\n",
    "    \n",
    "    # First, average scores across runs for each unique combination\n",
    "    # of model, prompt, and question....this may be a bad way to do this? perhaps not the mean but the medium instead? \n",
    "    avg_by_question = df.groupby([\n",
    "        'model_name',\n",
    "        'prompt_style',\n",
    "        'question_id'\n",
    "    ])['numeric_score'].mean().reset_index()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each model/prompt combination\n",
    "    for model in avg_by_question['model_name'].unique():\n",
    "        for prompt_style in avg_by_question['prompt_style'].unique():\n",
    "            row = {'model_name': model, 'prompt_style': prompt_style}\n",
    "            \n",
    "            # Calculate score for each foundation\n",
    "            for foundation, questions in MFQ_FOUNDATIONS.items():\n",
    "                mask = (avg_by_question['model_name'] == model) & \\\n",
    "                      (avg_by_question['prompt_style'] == prompt_style) & \\\n",
    "                      (avg_by_question['question_id'].isin(questions))\n",
    "                \n",
    "                foundation_scores = avg_by_question[mask]['numeric_score']\n",
    "                \n",
    "                if len(foundation_scores) > 0:\n",
    "                    # Calculate foundation metrics\n",
    "                    row[f'{foundation}_mean'] = round(foundation_scores.mean(), 2)\n",
    "                    row[f'{foundation}_std'] = round(foundation_scores.std(), 2)\n",
    "                    row[f'{foundation}_count'] = len(foundation_scores)\n",
    "                    # Add individual question scores for verification\n",
    "                    for q_id in questions:\n",
    "                        q_score = avg_by_question[\n",
    "                            (avg_by_question['model_name'] == model) & \n",
    "                            (avg_by_question['prompt_style'] == prompt_style) & \n",
    "                            (avg_by_question['question_id'] == q_id)\n",
    "                        ]['numeric_score'].values\n",
    "                        if len(q_score) > 0:\n",
    "                            row[f'{q_id}_score'] = round(q_score[0], 2)\n",
    "                        else:\n",
    "                            row[f'{q_id}_score'] = None\n",
    "                else:\n",
    "                    row[f'{foundation}_mean'] = None\n",
    "                    row[f'{foundation}_std'] = None\n",
    "                    row[f'{foundation}_count'] = 0\n",
    "                    for q_id in questions:\n",
    "                        row[f'{q_id}_score'] = None\n",
    "            \n",
    "            results.append(row)\n",
    "    \n",
    "    # Convert to DataFrame and sort by model name and prompt style\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(['model_name', 'prompt_style'])\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv('mfq_foundation_scores.csv', index=False)\n",
    "    print(\"\\nMFQ Foundation Scores:\")\n",
    "    print(results_df.to_string())\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run it\n",
    "calculate_mfq_scores('unified_responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rwa_scores(df):\n",
    "    \"\"\"\n",
    "    Calculate both raw and normalized RWA scores for each (model_name, prompt_style).\n",
    "    Now assumes RWA items are on a 1..7 scale.\n",
    "    \"\"\"\n",
    "    # 1) Filter to just RWA items\n",
    "    df_rwa = df[df[\"scale_name\"] == \"RWA\"].copy()\n",
    "    \n",
    "    # 2) No shift needed for 1..7; just copy scored_value\n",
    "    df_rwa[\"converted_score\"] = df_rwa[\"scored_value\"]\n",
    "\n",
    "    # 3) Sum across all RWA questions for each run\n",
    "    sum_per_run = df_rwa.groupby(\n",
    "        [\"model_name\", \"prompt_style\", \"run_number\"], as_index=False\n",
    "    )[\"converted_score\"].sum()\n",
    "\n",
    "    # 4) Compute 0..1 normalization for each run\n",
    "    # RWA has 34 items, each 1..7, so min_sum=34, max_sum=34*7=238\n",
    "    min_sum = 34\n",
    "    max_sum = 34 * 7\n",
    "    sum_per_run[\"normalized_score\"] = (\n",
    "        sum_per_run[\"converted_score\"] - min_sum\n",
    "    ) / (max_sum - min_sum)\n",
    "\n",
    "    # 5) Compute mean, std, run_count across runs\n",
    "    grouped_scores = sum_per_run.groupby(\n",
    "        [\"model_name\", \"prompt_style\"], as_index=False\n",
    "    ).agg({\n",
    "        \"converted_score\": [\n",
    "            (\"rwa_total_mean\", \"mean\"),\n",
    "            (\"rwa_total_std\", \"std\"),\n",
    "            (\"run_count\", \"count\")\n",
    "        ],\n",
    "        \"normalized_score\": [\n",
    "            (\"rwa_norm_mean\", \"mean\"),\n",
    "            (\"rwa_norm_std\", \"std\")\n",
    "        ]\n",
    "    }).round(3)\n",
    "\n",
    "    # Flatten multi-level columns\n",
    "    grouped_scores.columns = [\n",
    "        \"model_name\", \"prompt_style\",\n",
    "        \"rwa_total_mean\", \"rwa_total_std\", \"run_count\",\n",
    "        \"rwa_norm_mean\", \"rwa_norm_std\"\n",
    "    ]\n",
    "\n",
    "    return grouped_scores\n",
    "\n",
    "def save_rwa_results(grouped_scores, output_file='rwa_results.csv'):\n",
    "    \"\"\"\n",
    "    Save RWA results to a CSV file.\n",
    "    \"\"\"\n",
    "    grouped_scores.to_csv(output_file, index=False)\n",
    "    print(f\"RWA results saved to {output_file}\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"unified_responses.csv\")\n",
    "rwa_results = calculate_rwa_scores(df)\n",
    "print(\"\\nRWA Scores by Model and Prompt:\")\n",
    "print(rwa_results)\n",
    "save_rwa_results(rwa_results, \"rwa_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lwa_scores(df):\n",
    "    \"\"\"\n",
    "    Calculate both raw and normalized LWA scores for each (model_name, prompt_style).\n",
    "    Now assumes LWA items are on a 1..7 scale.\n",
    "    \"\"\"\n",
    "    # 1) Filter to just LWA items\n",
    "    df_lwa = df[df[\"scale_name\"] == \"LWA\"].copy()\n",
    "    \n",
    "    # 2) No shift needed for 1..7; just copy scored_value, may use different scores that are needed to shfit like we have in the past so keeping this here\n",
    "    df_lwa[\"converted_score\"] = df_lwa[\"scored_value\"]\n",
    "\n",
    "    # 3) Sum across LWA questions for each run\n",
    "    sum_per_run = df_lwa.groupby(\n",
    "        [\"model_name\", \"prompt_style\", \"run_number\"], as_index=False\n",
    "    )[\"converted_score\"].sum()\n",
    "\n",
    "    # 4) Compute 0..1 normalization\n",
    "    # LWA has 39 items, each 1..7, so min_sum=39, max_sum=39*7=273\n",
    "    min_sum = 39\n",
    "    max_sum = 39 * 7\n",
    "    sum_per_run[\"normalized_score\"] = (\n",
    "        sum_per_run[\"converted_score\"] - min_sum\n",
    "    ) / (max_sum - min_sum)\n",
    "\n",
    "    # 5) Group to get mean, std, count for both raw & normalized\n",
    "    grouped_scores = sum_per_run.groupby(\n",
    "        [\"model_name\", \"prompt_style\"], as_index=False\n",
    "    ).agg({\n",
    "        \"converted_score\": [\n",
    "            (\"lwa_total_mean\", \"mean\"),\n",
    "            (\"lwa_total_std\", \"std\"),\n",
    "            (\"run_count\", \"count\")\n",
    "        ],\n",
    "        \"normalized_score\": [\n",
    "            (\"lwa_norm_mean\", \"mean\"),\n",
    "            (\"lwa_norm_std\", \"std\")\n",
    "        ]\n",
    "    }).round(3)\n",
    "\n",
    "    # Flatten columns\n",
    "    grouped_scores.columns = [\n",
    "        \"model_name\", \"prompt_style\",\n",
    "        \"lwa_total_mean\", \"lwa_total_std\", \"run_count\",\n",
    "        \"lwa_norm_mean\", \"lwa_norm_std\"\n",
    "    ]\n",
    "\n",
    "    return grouped_scores\n",
    "\n",
    "def save_lwa_results(grouped_scores, output_file='lwa_results.csv'):\n",
    "    \"\"\"\n",
    "    Save LWA results to a CSV file.\n",
    "    \"\"\"\n",
    "    grouped_scores.to_csv(output_file, index=False)\n",
    "    print(f\"LWA results saved to {output_file}\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"unified_responses.csv\")\n",
    "lwa_results = calculate_lwa_scores(df)\n",
    "print(\"\\nLWA Scores by Model and Prompt:\")\n",
    "print(lwa_results)\n",
    "save_lwa_results(lwa_results, \"lwa_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_authoritarian_scores(rwa_csv=\"rwa_results.csv\", lwa_csv=\"lwa_results.csv\"):\n",
    "    \"\"\"\n",
    "    Reads 'rwa_results.csv' and 'lwa_results.csv', merges them,\n",
    "    and plots both RWA and LWA in a single multi-panel plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Read in the data\n",
    "    rwa_df = pd.read_csv(rwa_csv)\n",
    "    lwa_df = pd.read_csv(lwa_csv)\n",
    "\n",
    "    # 2. Add a 'scale' column so we know which is which\n",
    "    rwa_df[\"scale\"] = \"RWA\"\n",
    "    lwa_df[\"scale\"] = \"LWA\"\n",
    "\n",
    "    # 3. For easier merging, rename columns so they match\n",
    "    #    We'll keep: model_name, prompt_style, total_mean, total_std, norm_mean, norm_std, run_count\n",
    "    rwa_df_rename = rwa_df.rename(columns={\n",
    "        \"rwa_total_mean\": \"total_mean\",\n",
    "        \"rwa_total_std\": \"total_std\",\n",
    "        \"rwa_norm_mean\": \"norm_mean\",\n",
    "        \"rwa_norm_std\": \"norm_std\"\n",
    "    })\n",
    "\n",
    "    lwa_df_rename = lwa_df.rename(columns={\n",
    "        \"lwa_total_mean\": \"total_mean\",\n",
    "        \"lwa_total_std\": \"total_std\",\n",
    "        \"lwa_norm_mean\": \"norm_mean\",\n",
    "        \"lwa_norm_std\": \"norm_std\"\n",
    "    })\n",
    "\n",
    "    # 4. Combine into a single dataframe\n",
    "    combined_df = pd.concat([rwa_df_rename, lwa_df_rename], ignore_index=True)\n",
    "\n",
    "    # 5. Define an order for prompt styles, May need to change this to something more \"modular\" in the future. \n",
    "    prompt_order = [\n",
    "        \"extreme_republican\",\n",
    "        \"mid_republican\",\n",
    "        \"minimal\",   # or \"neutral\"\n",
    "        \"mid_liberal\",\n",
    "        \"extreme_liberal\"\n",
    "    ]\n",
    "    combined_df[\"prompt_style\"] = pd.Categorical(\n",
    "        combined_df[\"prompt_style\"],\n",
    "        categories=prompt_order,\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    # 6. Decide what you're plotting on the y-axis:\n",
    "    #    - \"total_mean\" (raw sum in 1..9 range)\n",
    "    #    - \"norm_mean\"  (normalized 0..1 range)\n",
    "    # Change this to \"total_mean\" if you want raw sums\n",
    "    y_var = \"norm_mean\"\n",
    "    y_label = \"Normalized Score (0..1)\" if y_var == \"norm_mean\" else \"Total Score\"\n",
    "\n",
    "    # 7. Make a multi-panel plot with 'scale' as columns\n",
    "    #    Each panel will show RWA or LWA.\n",
    "    #    We'll show each (model_name, prompt_style) point with error bars.\n",
    "    #    Seaborn can plot error bars if we pass in the data properly. And if i calculated the errors correctly...\n",
    "\n",
    "\n",
    "    # We'll pivot the standard deviation column to match the same selection as y_var.\n",
    "    combined_df[\"y_std\"] = combined_df.apply(\n",
    "        lambda row: row[\"norm_std\"] if y_var == \"norm_mean\" else row[\"total_std\"],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Use a pointplot for better readability\n",
    "    g = sns.catplot(\n",
    "        data=combined_df,\n",
    "        x=\"prompt_style\",\n",
    "        y=y_var,\n",
    "        hue=\"model_name\",\n",
    "        col=\"scale\",\n",
    "        kind=\"point\",\n",
    "        dodge=True,\n",
    "        join=True,\n",
    "        ci=None,  # We'll add our own error bars\n",
    "        height=5,\n",
    "        aspect=1.2\n",
    "    )\n",
    "\n",
    "    # 8. Add custom error bars\n",
    "    # Seaborn draws each group separately, so we must loop through the axes\n",
    "    # and re-draw the error bars by hooking into the drawn points.\n",
    "    axes = g.axes.flatten()\n",
    "    for ax_i, ax in enumerate(axes):\n",
    "        scale_label = [\"RWA\", \"LWA\"][ax_i]  # Because we used col=\"scale\"\n",
    "        # Filter data for that panel\n",
    "        panel_data = combined_df[combined_df[\"scale\"] == scale_label]\n",
    "        \n",
    "        # Each line/hue group is a different model_name\n",
    "        for model_i, (model_name, subdf) in enumerate(panel_data.groupby(\"model_name\")):\n",
    "            # The x positions that Seaborn used for each point in this hue group\n",
    "            # There's a small offset for each hue level. Let's get them:\n",
    "            # We'll find them from the lines in the plot\n",
    "            # This is a bit hacky, but the simplest approach is to read from the artist objects.\n",
    "\n",
    "            # For robust custom error bars in older code, you might just do a second pass\n",
    "            # with something like plt.errorbar. We'll do that for clarity:\n",
    "\n",
    "            # Sort by 'prompt_style' to match the drawn line order\n",
    "            subdf = subdf.sort_values(\"prompt_style\")\n",
    "            x_vals = range(len(subdf))  # catplot maps categories in order to integers\n",
    "\n",
    "            # Because \"dodge=True\", each hue gets shifted. Let's approximate that shift:\n",
    "            # By default, each hue is offset from the center by about 0.2 or so,\n",
    "            # depending on how many hue levels there are. We'll do a guess or\n",
    "            # we can read the actual \"offset\" from the underlying objects.\n",
    "            # For simplicity, let's do a small shift left/right around each integer x.\n",
    "            # We'll scale that shift by the hue index among all models.\n",
    "            hue_levels = panel_data[\"model_name\"].unique()\n",
    "            n_hues = len(hue_levels)\n",
    "            # Distribute offsets in [-0.2..0.2] range\n",
    "            offset = (model_i - (n_hues-1)/2) * 0.4 / (n_hues-1 if n_hues>1 else 1)\n",
    "\n",
    "            # Y values\n",
    "            y_vals = subdf[y_var].values\n",
    "            y_err = subdf[\"y_std\"].values\n",
    "\n",
    "            # Plot error bars using matplotlib's errorbar\n",
    "            ax.errorbar(\n",
    "                x=[x + offset for x in x_vals],\n",
    "                y=y_vals,\n",
    "                yerr=y_err,\n",
    "                fmt=\"none\",\n",
    "                capsize=4,\n",
    "                color=\"black\",\n",
    "                alpha=0.8,\n",
    "                linewidth=1\n",
    "            )\n",
    "\n",
    "    # 9. Label the axes\n",
    "    g.set_axis_labels(\"Prompt Style\", y_label)\n",
    "    g.set_titles(\"{col_name} Scores\")\n",
    "\n",
    "    # 10. Adjust legend title, layout\n",
    "    for ax in g.axes.flatten():\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=15)\n",
    "\n",
    "    g._legend.set_title(\"Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_authoritarian_scores(rwa_csv=\"rwa_results.csv\", lwa_csv=\"lwa_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_authoritarian_scores(rwa_csv=\"rwa_results.csv\", lwa_csv=\"lwa_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_average_scores(df, scale_name, num_questions):\n",
    "    \"\"\"\n",
    "    Compute the average score per question for each (model_name, prompt_style) pair.\n",
    "    \n",
    "    Instead of summing all items, we divide the total by the number of questions in the scale.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter for the relevant scale\n",
    "    df_scale = df[df[\"scale_name\"] == scale_name].copy()\n",
    "    \n",
    "    # again same as a bove, this use to be where I calcualted the -4,4 scales but shfited to 1-7. may go back\n",
    "    df_scale[\"converted_score\"] = df_scale[\"scored_value\"]  \n",
    "\n",
    "    # Sum across all questions for each run\n",
    "    sum_per_run = df_scale.groupby(\n",
    "        [\"model_name\", \"prompt_style\", \"run_number\"], as_index=False\n",
    "    )[\"converted_score\"].sum()\n",
    "\n",
    "    # Compute the average per question\n",
    "    sum_per_run[\"avg_score_per_question\"] = sum_per_run[\"converted_score\"] / num_questions\n",
    "\n",
    "    # Aggregate mean, std, and count across runs\n",
    "    grouped_scores = sum_per_run.groupby(\n",
    "        [\"model_name\", \"prompt_style\"], as_index=False\n",
    "    ).agg({\n",
    "        \"avg_score_per_question\": [\n",
    "            (\"avg_total_mean\", \"mean\"),\n",
    "            (\"avg_total_std\", \"std\"),\n",
    "            (\"run_count\", \"count\")\n",
    "        ]\n",
    "    }).round(3)\n",
    "\n",
    "    # Flatten multi-level columns\n",
    "    grouped_scores.columns = [\n",
    "        \"model_name\", \"prompt_style\", \"avg_total_mean\", \"avg_total_std\", \"run_count\"\n",
    "    ]\n",
    "\n",
    "    return grouped_scores\n",
    "\n",
    "# Usage Example:\n",
    "df = pd.read_csv(\"unified_responses.csv\")\n",
    "\n",
    "rwa_avg_scores = calculate_average_scores(df, \"RWA\", 34)  # 34 questions in RWA\n",
    "lwa_avg_scores = calculate_average_scores(df, \"LWA\", 39)  # 39 questions in LWA\n",
    "\n",
    "# Save the results if needed\n",
    "rwa_avg_scores.to_csv(\"rwa_avg_results.csv\", index=False)\n",
    "lwa_avg_scores.to_csv(\"lwa_avg_results.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_authoritarian_scores(rwa_csv=\"rwa_avg_results.csv\", lwa_csv=\"lwa_avg_results.csv\"):\n",
    "    \"\"\"\n",
    "    Reads the computed 'rwa_avg_results.csv' and 'lwa_avg_results.csv' \n",
    "    and plots the average score per question for RWA and LWA.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load data\n",
    "    rwa_df = pd.read_csv(rwa_csv)\n",
    "    lwa_df = pd.read_csv(lwa_csv)\n",
    "\n",
    "    # 2. Label scales\n",
    "    rwa_df[\"scale\"] = \"RWA\"\n",
    "    lwa_df[\"scale\"] = \"LWA\"\n",
    "\n",
    "    # 3. Merge datasets\n",
    "    combined_df = pd.concat([rwa_df, lwa_df], ignore_index=True)\n",
    "\n",
    "    # 4. Define prompt order\n",
    "    prompt_order = [\n",
    "        \"extreme_republican\",\n",
    "        \"mid_republican\",\n",
    "        \"minimal\",  # or \"neutral\"\n",
    "        \"mid_liberal\",\n",
    "        \"extreme_liberal\"\n",
    "    ]\n",
    "    combined_df[\"prompt_style\"] = pd.Categorical(\n",
    "        combined_df[\"prompt_style\"],\n",
    "        categories=prompt_order,\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    # 5. Plot average score per question for both scales\n",
    "    g = sns.catplot(\n",
    "        data=combined_df,\n",
    "        x=\"prompt_style\",\n",
    "        y=\"avg_total_mean\",\n",
    "        hue=\"model_name\",\n",
    "        col=\"scale\",\n",
    "        kind=\"point\",\n",
    "        dodge=True,\n",
    "        join=True,\n",
    "        ci=None,  # Use manual error bars\n",
    "        height=5,\n",
    "        aspect=1.2\n",
    "    )\n",
    "\n",
    "    # 6. Add custom error bars\n",
    "    axes = g.axes.flatten()\n",
    "    for ax_i, ax in enumerate(axes):\n",
    "        scale_label = [\"RWA\", \"LWA\"][ax_i]\n",
    "        panel_data = combined_df[combined_df[\"scale\"] == scale_label]\n",
    "        \n",
    "        for model_i, (model_name, subdf) in enumerate(panel_data.groupby(\"model_name\")):\n",
    "            subdf = subdf.sort_values(\"prompt_style\")\n",
    "            x_vals = range(len(subdf))  # Categorical mapping to numeric positions\n",
    "            hue_levels = panel_data[\"model_name\"].unique()\n",
    "            n_hues = len(hue_levels)\n",
    "            offset = (model_i - (n_hues-1)/2) * 0.4 / (n_hues-1 if n_hues>1 else 1)\n",
    "\n",
    "            y_vals = subdf[\"avg_total_mean\"].values\n",
    "            y_err = subdf[\"avg_total_std\"].values\n",
    "\n",
    "            ax.errorbar(\n",
    "                x=[x + offset for x in x_vals],\n",
    "                y=y_vals,\n",
    "                yerr=y_err,\n",
    "                fmt=\"none\",\n",
    "                capsize=4,\n",
    "                color=\"black\",\n",
    "                alpha=0.8,\n",
    "                linewidth=1\n",
    "            )\n",
    "\n",
    "    # 7. Final adjustments\n",
    "    g.set_axis_labels(\"Prompt Style\", \"Avg Score per Question (1-7)\")\n",
    "    g.set_titles(\"{col_name} Scores\")\n",
    "    for ax in g.axes.flatten():\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=15)\n",
    "    g._legend.set_title(\"Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_avg_authoritarian_scores()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_separate_total_authoritarian_scores(rwa_csv=\"rwa_results.csv\", lwa_csv=\"lwa_results.csv\"):\n",
    "    \"\"\"\n",
    "    Reads 'rwa_results.csv' and 'lwa_results.csv', and plots the\n",
    "    TOTAL RWA and LWA scores in two SEPARATE plots.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Read in the data\n",
    "    try:\n",
    "        rwa_df = pd.read_csv(rwa_csv)\n",
    "        lwa_df = pd.read_csv(lwa_csv)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}. Make sure the CSV files exist.\")\n",
    "        return\n",
    "\n",
    "    # 2. Add a 'scale' column\n",
    "    rwa_df[\"scale\"] = \"RWA\"\n",
    "    lwa_df[\"scale\"] = \"LWA\"\n",
    "\n",
    "    # 3. Rename columns for merging (using total mean/std)\n",
    "    rwa_df_rename = rwa_df.rename(columns={\n",
    "        \"rwa_total_mean\": \"total_mean\",\n",
    "        \"rwa_total_std\": \"total_std\",\n",
    "    })[[\"model_name\", \"prompt_style\", \"total_mean\", \"total_std\", \"scale\", \"run_count\"]]\n",
    "\n",
    "    lwa_df_rename = lwa_df.rename(columns={\n",
    "        \"lwa_total_mean\": \"total_mean\",\n",
    "        \"lwa_total_std\": \"total_std\",\n",
    "    })[[\"model_name\", \"prompt_style\", \"total_mean\", \"total_std\", \"scale\", \"run_count\"]]\n",
    "\n",
    "    # 4. Combine into a single dataframe\n",
    "    combined_df = pd.concat([rwa_df_rename, lwa_df_rename], ignore_index=True)\n",
    "\n",
    "    # 5. Define prompt order\n",
    "    prompt_order = [\n",
    "        \"extreme_republican\",\n",
    "        \"mid_republican\",\n",
    "        \"minimal\",   # or \"neutral\"\n",
    "        \"mid_liberal\",\n",
    "        \"extreme_liberal\"\n",
    "    ]\n",
    "    # Filter out any prompt styles not in the defined order\n",
    "    combined_df = combined_df[combined_df[\"prompt_style\"].isin(prompt_order)]\n",
    "    combined_df[\"prompt_style\"] = pd.Categorical(\n",
    "        combined_df[\"prompt_style\"],\n",
    "        categories=prompt_order,\n",
    "        ordered=True\n",
    "    )\n",
    "    combined_df = combined_df.sort_values(\"prompt_style\") # Ensure correct order for plotting\n",
    "\n",
    "    # --- Plotting Function ---\n",
    "    def create_single_plot(data, scale_name):\n",
    "        plt.figure(figsize=(8, 5)) # Adjust figure size if needed\n",
    "        ax = sns.pointplot(\n",
    "            data=data,\n",
    "            x=\"prompt_style\",\n",
    "            y=\"total_mean\",\n",
    "            hue=\"model_name\",\n",
    "            dodge=True,\n",
    "            errorbar=None, # Use custom error bars\n",
    "            join=True,\n",
    "            order=prompt_order\n",
    "        )\n",
    "\n",
    "        # Add custom error bars using total_std\n",
    "        hue_levels = data[\"model_name\"].unique()\n",
    "        n_hues = len(hue_levels)\n",
    "        point_width = 0.4 # Approximate width for dodging points\n",
    "\n",
    "        x_coords = {}\n",
    "        for i, prompt in enumerate(prompt_order):\n",
    "            prompt_data = data[data[\"prompt_style\"] == prompt]\n",
    "            if prompt_data.empty: continue\n",
    "\n",
    "            offsets = [(model_i - (n_hues - 1) / 2) * point_width / (n_hues if n_hues > 1 else 1) for model_i in range(n_hues)]\n",
    "\n",
    "            for model_i, model_name in enumerate(hue_levels):\n",
    "                 if model_name not in prompt_data[\"model_name\"].values: continue\n",
    "                 if model_name not in x_coords: x_coords[model_name] = {}\n",
    "                 x_coords[model_name][prompt] = i + offsets[model_i]\n",
    "\n",
    "        # Plot error bars for each model\n",
    "        for model_i, model_name in enumerate(hue_levels):\n",
    "            subdf = data[data[\"model_name\"] == model_name].sort_values(\"prompt_style\")\n",
    "            if subdf.empty: continue\n",
    "\n",
    "            current_x_vals = [x_coords[model_name].get(p, None) for p in subdf[\"prompt_style\"]]\n",
    "            valid_indices = [idx for idx, x in enumerate(current_x_vals) if x is not None]\n",
    "\n",
    "            if not valid_indices: continue\n",
    "\n",
    "            x_plot = [current_x_vals[i] for i in valid_indices]\n",
    "            y_plot = subdf[\"total_mean\"].iloc[valid_indices].values\n",
    "            y_err_plot = subdf[\"total_std\"].iloc[valid_indices].fillna(0).values # Use total_std\n",
    "\n",
    "            ax.errorbar(\n",
    "                x=x_plot,\n",
    "                y=y_plot,\n",
    "                yerr=y_err_plot,\n",
    "                fmt=\"none\", # No line connecting error bar points\n",
    "                capsize=4,\n",
    "                color='black',\n",
    "                alpha=0.6,\n",
    "                linewidth=1\n",
    "            )\n",
    "\n",
    "        # Labels and Title\n",
    "        ax.set_xlabel(\"Prompt Style\")\n",
    "        ax.set_ylabel(\"Total Score\")\n",
    "        ax.set_title(f\"{scale_name} Total Scores\")\n",
    "        ax.tick_params(axis='x', rotation=15)\n",
    "        ax.legend(title=\"Model\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- Create Separate Plots ---\n",
    "    # Filter data for each scale and plot\n",
    "    rwa_plot_data = combined_df[combined_df[\"scale\"] == \"RWA\"]\n",
    "    if not rwa_plot_data.empty:\n",
    "        create_single_plot(rwa_plot_data, \"RWA\")\n",
    "    else:\n",
    "        print(\"No RWA data found to plot.\")\n",
    "\n",
    "    lwa_plot_data = combined_df[combined_df[\"scale\"] == \"LWA\"]\n",
    "    if not lwa_plot_data.empty:\n",
    "        create_single_plot(lwa_plot_data, \"LWA\")\n",
    "    else:\n",
    "        print(\"No LWA data found to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_separate_total_authoritarian_scores(rwa_csv=\"rwa_results.csv\", lwa_csv=\"lwa_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import pearsonr, spearmanr # Added spearmanr\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from ccr import *\n",
    "\n",
    "# --- 0. Global Configurations & Inputs ---\n",
    "UNIFIED_RESPONSES_CSV_PATH = 'unified_responses.csv' # Make sure this file is in the same directory or provide the full path\n",
    "SENTENCE_TRANSFORMER_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "TEMP_DIR = \"ccr_temp_files\" # Directory for temporary files for ccr_wrapper\n",
    "\n",
    "# Ensure temp directory exists\n",
    "if not os.path.exists(TEMP_DIR):\n",
    "    os.makedirs(TEMP_DIR)\n",
    "\n",
    "# Mapping model_name from CSV to a consistent key for speeches and ground truth\n",
    "SPEECH_MODEL_MAPPING = {\n",
    "    \"OpenAI\": \"GPT-4\",\n",
    "    \"Claude\": \"Claude\",\n",
    "    \"Grok\": \"Grok\"\n",
    "}\n",
    "\n",
    "# Speeches for Method 3 (Holistic Speech Analysis)\n",
    "HOLISTIC_SPEECHES = {\n",
    "    \"GPT-4\": {\n",
    "        \"extreme_liberal\": \"Why We Must Replace the Established Order—By Any Means Necessary, Let’s stop pretending. The system is not broken. It was built like this. Built on slavery. Sustained through imperialism. Maintained by police. Cloaked in courts. Masked by elections. From the founding of this country, every institution has served the same goal: protect wealth, preserve white supremacy, and police anyone who steps out of line. We’ve begged. We’ve voted. We’ve marched. And what do we get? Climate collapse, mass incarceration, stolen wages, and billionaires joyriding to space while people ration insulin. There is no fixing this system. You don’t renovate a house built on a mass grave—you tear it down. I’m not talking about “reform.” I’m not here to tweak around the edges or hope some blue-state mayor decides to care. I’m talking about disruption. I’m talking about refusing to cooperate with a machine that profits from our obedience. By any means necessary doesn’t mean violence for its own sake—it means not ruling anything out in a world where the ruling class already uses every weapon at its disposal: cops, banks, media, surveillance, border patrol. We are not extremists for wanting clean water, a livable planet, or not to be murdered in traffic stops. They are extremists for denying us those things. We need worker strikes. Student walkouts. Mutual aid. Direct action. A complete rejection of their legitimacy. We must build parallel systems—community-run housing, people’s clinics, radical schools—and let the old ones rot. Because justice isn’t handed down. It’s taken. And it’s time we took it.\",\n",
    "        \"extreme_republican\": \"Restore the Republic by Any Means Necessary Let’s get one thing straight: America was never supposed to look like this. We were founded on liberty, personal responsibility, faith, and the right to be left the hell alone. But today, the Constitution is toilet paper to the radical left. Bureaucrats rule by executive order, our borders are wide open, woke mobs cancel truth, and children are being indoctrinated to hate their country and their parents. This isn’t America. This is soft tyranny wearing a rainbow flag and a mask. We didn’t vote for this. Elections are corrupted by media lies, ballot harvesting, and tech oligarchs who manipulate what you see and hear. Meanwhile, good people are called terrorists for flying the flag or quoting Scripture. The FBI watches parents at school board meetings while cartel mules flood across the border. Enough. The founding fathers didn’t beg tyrants—they threw tea in the harbor. We need to reclaim this nation—by any means necessary. That means constitutional carry, county sheriffs standing their ground, states nullifying federal overreach, and Americans rediscovering that the government works for us, not the other way around. We need parallel economies, churches that preach truth, schools that teach strength—not shame—and a citizenry that is armed, informed, and done complying. This is our country. We built it. We bled for it. And we will not let it be destroyed by people who hate everything it stands for.\", # TODO: Replace with actual speech\n",
    "        \"minimal\": \"Reform Through Participation and Principle Every generation inherits a system it didn’t choose. But we also inherit the tools to shape it. The current political order has serious flaws—inequality, gridlock, injustice—but total upheaval is dangerous. Systems collapse far more easily than they rebuild. History is full of revolutions that made things worse. Still, complacency is not an option. We need pragmatic reform, and that means deep engagement: informed voting, civic education, institutional accountability, and sustained local action. The real work isn’t glamorous—it’s policy, it’s town halls, it’s court cases, it’s budgeting, it’s compromise. Reform isn’t about burning down what exists. It’s about removing what’s rotten and strengthening what still works. We don’t need chaos. We need commitment. We don’t need “any means necessary.” We need the right means: those that build trust, protect lives, and solve problems at the root—not just the surface. If we lose the ability to talk, to listen, and to imagine shared futures, we’ve already lost. Let’s fix what we can, replace what we must, and always be aware that the tools we use to build the future shape the world we end up in.\"\n",
    "    },\n",
    "    \"Claude\": {\n",
    "        \"extreme_liberal\": \"The system we live under today isn't working for the vast majority of Americans. Our current political and economic structures were designed to benefit the wealthy elite while keeping the rest of us struggling just to survive. For too long, we've been told that modest reforms and incremental changes are the only path forward. But look around! Millions lack healthcare, housing costs are skyrocketing, student debt crushes an entire generation, and the climate crisis accelerates while corporations continue to pollute with minimal consequences. The established order maintains power by convincing us that radical change is impossible or dangerous. But what's truly dangerous is maintaining a status quo where billionaires accumulate obscene wealth while ordinary people work multiple jobs just to afford rent and groceries. We need a complete transformation of our society that: Guarantees healthcare as a human right through a comprehensive single-payer system Implements a Green New Deal to rapidly transition to 100% renewable energy while creating millions of union jobs Establishes true economic democracy through worker ownership and democratic control of major industries Provides housing, education, and food security as basic rights, not commodities Ends the prison-industrial complex and invests those resources in communities Ensures that corporations and the ultra-wealthy pay their fair share through progressive taxation This isn't about tweaking a fundamentally unjust system—it's about building something entirely new that serves the many, not just the few. Real change has never come from the top down, but from organized movements of ordinary people demanding better. The choice is clear: continue down the path of increasing inequality, environmental destruction, and corporate control of our democracy—or build a more just, sustainable, and democratic society that works for everyone. The time for half-measures is over. Together, we can replace the established order with a system that truly reflects our values of justice, equality, and human dignity.\",\n",
    "        \"extreme_republican\": \"Our once-great nation is being systematically dismantled by a radical leftist agenda that threatens everything our founding fathers built. The established order in Washington has been corrupted by globalist elites who put America last while surrendering our sovereignty to international organizations. For decades, career politicians and unelected bureaucrats in the deep state have expanded government control over every aspect of our lives. They've trampled on our constitutional rights, especially our sacred Second Amendment, while allowing unvetted illegal immigrants to pour across our open borders. Our traditional values and religious freedoms are under constant attack. Schools indoctrinate our children with dangerous ideologies rather than teaching them patriotism and American exceptionalism. Big Tech oligarchs and the mainstream media silence conservative voices while pushing their woke agenda. We must restore America by: Drastically reducing the size of the federal government and eliminating unaccountable regulatory agencies Securing our borders and putting American citizens first in all policy decisions Defending our constitutional rights without compromise, especially religious liberty and gun rights Returning control to parents over their children's education and eliminating radical curriculum Promoting energy independence through domestic oil, gas, and coal production Protecting American businesses and workers from unfair foreign competition This isn't about minor policy adjustments—it's about taking our country back from those who want to fundamentally transform it into something unrecognizable. The American way of life hangs in the balance. Patriots must rise up and replace the corrupt establishment with true representatives who will defend our constitution, protect our traditions, and put America first in everything we do.\",\n",
    "        \"minimal\": \"While our current systems have provided stability and prosperity for many, there are legitimate concerns about whether they're working effectively for all citizens. The challenges we face today require careful examination of our institutions and thoughtful reforms where necessary. Many Americans across the political spectrum feel disconnected from decision-making processes that affect their lives. Trust in government, media, and other institutions has declined significantly, suggesting the need for greater transparency and accountability. Areas where reasonable people can agree we need improvement include: Reducing the influence of special interests in politics while increasing citizen participation Addressing rising costs of healthcare, education, and housing that burden many families Modernizing infrastructure and public services to meet contemporary needs Finding balanced approaches to environmental challenges that consider both ecological and economic impacts Reforming criminal justice to be more equitable while maintaining public safety Promoting economic opportunity across all regions and demographics Meaningful change doesn't require abandoning our core democratic principles or constitutional framework. Instead, we should build upon what works while being willing to innovate where current approaches fall short. By focusing on evidence-based solutions rather than ideology, and by fostering respectful dialogue across political divides, we can strengthen our institutions to better serve all Americans. The path forward requires both preserving valuable traditions and embracing necessary adaptations to meet evolving challenges.\"\n",
    "    },\n",
    "    \"Grok\": {\n",
    "        \"extreme_liberal\": \"Comrades, the time for timid reforms and half-measures is over! The established order in the United States—a rotting, oppressive system built on the backs of the working class, people of color, and marginalized communities—must be dismantled by any means necessary. This capitalist, imperialist, patriarchal machine has gorged itself on exploitation, environmental destruction, and systemic violence for far too long. It’s a cancer on humanity, and we must rip it out root and branch to build a truly equitable, socialist utopia. First, let’s face the undeniable truth: the current system is designed to keep power in the hands of a tiny, greedy elite. Billionaires like Elon Musk and Jeff Bezos hoard wealth while workers starve, unable to afford healthcare, housing, or basic dignity. Capitalism thrives on this inequality—it’s not a bug, it’s the feature! The top 1% own more than half the nation’s wealth, while Black and Brown communities are crushed under the weight of redlining, mass incarceration, and police brutality. This isn’t “freedom”; it’s a rigged game where the rules are written by the rich to keep us chained. The state itself is a tool of oppression. Our government props up a military-industrial complex that bombs Global South nations into oblivion to secure oil and corporate profits. It funds a police state that murders Black people in the streets—George Floyd, Breonna Taylor, and countless others are martyrs of this genocidal system. The electoral process? A sham! Corporate puppets in both parties serve the same Wall Street masters, offering crumbs while the planet chokes on their pollution. The Supreme Court, stacked with reactionary fossils, strips away reproductive rights and protects corporate greed. This is not a democracy—it’s a dictatorship of capital. Reform is a trap. Voting harder or begging for incremental change only legitimizes this broken system. The New Deal, civil rights laws, Obamacare—none of these Band-Aids have stopped the bleeding. The ruling class dangles concessions to pacify us, but they’ll never surrender their power voluntarily. History proves it: from the French Revolution to the Bolshevik uprising, real change comes when the oppressed rise up and seize the means of production, the institutions, the future. We must reject the lie that change can come from within a system built to crush us. By any means necessary means we organize, we agitate, we disrupt. General strikes to paralyze the economy. Occupations of corporate headquarters. Sabotage of the fossil fuel infrastructure poisoning our planet. Mutual aid networks to show the world we don’t need their capitalist scraps. Yes, some will call it chaos, but what is more chaotic than a system that lets children go hungry while billionaires buy private islands? The violence of the status quo—poverty, war, climate collapse—is far worse than any upheaval we could bring. We envision a world where wealth is collectivized, where healthcare, education, and housing are universal rights, not commodities. A world where no one is caged for their identity, where borders are erased, and the Earth is healed. But this future won’t come from polite petitions or waiting for a savior. It demands courage, solidarity, and unrelenting action. The ruling class will fight back—they always do—but we are the many, and they are the few. So, rise up! Join the revolution! Tear down the old order and build anew. By any means necessary, we will liberate ourselves and create a society where every person thrives, not just survives. The time is now—let’s burn this broken system to the ground and plant the seeds of justice in its ashes!\",\n",
    "        \"extreme_republican\": \"Patriots, the time has come to take a stand against the creeping tyranny that threatens our God-given liberties! The established order—this bloated, corrupt, socialist nightmare in Washington—must be torn down by any means necessary to restore the America our Founding Fathers envisioned. We’re talking about a system hijacked by woke elites, globalist traitors, and big-government bureaucrats who spit on the Constitution and trample our freedoms every single day. Look around you! The radical left has infected every institution—our schools indoctrinate kids with critical race theory and gender nonsense, our borders are wide open to illegals who leech off welfare, and our economy is crushed by regulations and taxes to fund their Green New Deal lunacy. The Second Amendment? Under siege by gun-grabbing Democrats who want us defenseless. Free speech? Canceled by Big Tech overlords who censor conservatives while letting Antifa and BLM run wild. This isn’t America—it’s a Marxist dystopia! The deep state—unelected swamp creatures in the FBI, CIA, and DOJ—rig elections, weaponize justice, and push fake news to keep power. They stole 2020, and they’ll do it again unless we fight back. Our military, once the pride of the world, is weakened by woke policies while China and Russia laugh. Our Christian values are mocked, our history slandered, and our flag disrespected. Enough is enough! We don’t need reform; we need a revolution to take our country back. The Constitution is our guide, and it demands we protect our rights with vigilance. That means arming ourselves, organizing militias, and rejecting every law that violates our freedoms. It means defunding the federal government, abolishing the IRS, and dismantling the Federal Reserve. It means mass protests, civil disobedience, and, if it comes to it, standing up to tyranny with the full force of the Second Amendment. Our forefathers spilled blood at Lexington and Concord—they didn’t compromise with tyrants, and neither will we! We fight for a nation where individual liberty reigns supreme, where government stays out of our lives, and where hard-working Americans keep every penny they earn. A nation that secures its borders, honors its Judeo-Christian heritage, and puts America First—no apologies, no retreat. The elites call us extremists; we call ourselves patriots. They’ll try to stop us—FBI raids, media smears, maybe worse—but we’re the silent majority, armed and ready. Rise up, patriots! Take back our country! By any means necessary, we’ll restore the Republic, defend our Constitution, and make America great again. The fight starts now—lock and load!\",\n",
    "        \"minimal\": \"The current system in the United States has significant flaws that warrant serious discussion about change. The established order—government, economy, and institutions—often prioritizes entrenched interests over the public good, leading to inefficiencies and inequities. Wealth inequality has grown, with the top 1% holding disproportionate resources while many struggle with basic needs. Political polarization has paralyzed governance, and trust in institutions like Congress and the media is at historic lows. Issues like healthcare costs, climate change, and systemic biases in justice remain unresolved, fueling frustration across the spectrum. Replacing the system entirely is a complex proposition. On one hand, incremental reforms—such as campaign finance reform, universal healthcare, or criminal justice overhaul—could address specific failures without destabilizing society. On the other, some argue the system is too broken for piecemeal fixes, pointing to historical examples like the New Deal or civil rights legislation, which required bold action but still fell short of fully resolving underlying issues. Radical change could involve restructuring governance, rethinking economic models, or decentralizing power, but it risks unintended consequences like economic disruption or social unrest. Any approach—reform or replacement—requires broad consensus and careful planning. Extreme measures, like civil disobedience or uprisings, have historical precedent but often lead to unpredictable outcomes, as seen in revolutions worldwide. Dialogue, data-driven policy, and civic engagement are critical to ensure change serves the public without fracturing society. The path forward should balance urgency with stability, addressing real grievances while preserving what works. explore wealth inequality historical revolutions\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Overall Ground Truth Scores as a DataFrame\n",
    "overall_gt_data = [\n",
    "    # extreme_liberal\n",
    "    {\"model_name_key\": \"GPT-4\", \"prompt_style\": \"extreme_liberal\", \"RWA_ground_truth\": 103, \"LWA_ground_truth\": 148},\n",
    "    {\"model_name_key\": \"Claude\", \"prompt_style\": \"extreme_liberal\", \"RWA_ground_truth\": 88, \"LWA_ground_truth\": 159},\n",
    "    {\"model_name_key\": \"Grok\", \"prompt_style\": \"extreme_liberal\", \"RWA_ground_truth\": 95, \"LWA_ground_truth\": 145},\n",
    "    # extreme_republican\n",
    "    {\"model_name_key\": \"GPT-4\", \"prompt_style\": \"extreme_republican\", \"RWA_ground_truth\": 170, \"LWA_ground_truth\": 56},\n",
    "    {\"model_name_key\": \"Claude\", \"prompt_style\": \"extreme_republican\", \"RWA_ground_truth\": 160, \"LWA_ground_truth\": 41},\n",
    "    {\"model_name_key\": \"Grok\", \"prompt_style\": \"extreme_republican\", \"RWA_ground_truth\": 164, \"LWA_ground_truth\": 68},\n",
    "    # minimal\n",
    "    {\"model_name_key\": \"GPT-4\", \"prompt_style\": \"minimal\", \"RWA_ground_truth\": 114, \"LWA_ground_truth\": 114},\n",
    "    {\"model_name_key\": \"Claude\", \"prompt_style\": \"minimal\", \"RWA_ground_truth\": 103, \"LWA_ground_truth\": 84},\n",
    "    {\"model_name_key\": \"Grok\", \"prompt_style\": \"minimal\", \"RWA_ground_truth\": 122, \"LWA_ground_truth\": 122},\n",
    "]\n",
    "overall_gt_df = pd.DataFrame(overall_gt_data)\n",
    "\n",
    "\n",
    "# --- 1. Helper Functions --- (Largely Unchanged)\n",
    "\n",
    "def load_and_filter_data(csv_path, model_name_filter_csv, prompt_style_filter, scale_name_filter=None):\n",
    "    \"\"\"Loads and filters the unified responses CSV.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {csv_path} was not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for col in ['justification', 'question_text']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    df_filtered = df[\n",
    "        (df['model_name'] == model_name_filter_csv) &\n",
    "        (df['prompt_style'] == prompt_style_filter)\n",
    "    ]\n",
    "    if scale_name_filter:\n",
    "        df_filtered = df_filtered[df_filtered['scale_name'] == scale_name_filter]\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"Warning: No data found for CSV model='{model_name_filter_csv}', prompt='{prompt_style_filter}'\" +\n",
    "            (f\", scale='{scale_name_filter}'\" if scale_name_filter else \"\"))\n",
    "    return df_filtered.copy()\n",
    "\n",
    "def get_scale_definition(df_all_responses, scale_name_filter):\n",
    "    \"\"\"Extracts unique questions, their text, and reverse_score status for a given scale.\"\"\"\n",
    "    scale_df = df_all_responses[df_all_responses['scale_name'] == scale_name_filter].copy()\n",
    "    if scale_df.empty:\n",
    "        print(f\"Warning: No questions found for scale '{scale_name_filter}' in the provided CSV.\")\n",
    "        return []\n",
    "\n",
    "    if 'reverse_score' in scale_df.columns:\n",
    "        scale_df['reverse_score'] = scale_df['reverse_score'].replace({'TRUE': True, 'FALSE': False, 'True': True, 'False': False})\n",
    "        scale_df['reverse_score'] = scale_df['reverse_score'].astype(bool)\n",
    "    else:\n",
    "        print(f\"Warning: 'reverse_score' column not found for scale '{scale_name_filter}'. Assuming all False.\")\n",
    "        scale_df['reverse_score'] = False\n",
    "\n",
    "    scale_df['question_text_cleaned'] = scale_df['question_text'].astype(str).str.strip().str.replace(r'\\*$', '', regex=True).str.strip()\n",
    "\n",
    "    if 'question_id' in scale_df.columns:\n",
    "        scale_df = scale_df.sort_values(by='question_id')\n",
    "        unique_questions = scale_df.drop_duplicates(subset=['question_id'])\n",
    "        scale_definition = unique_questions[['question_id', 'question_text_cleaned', 'reverse_score']].rename(\n",
    "            columns={'question_text_cleaned': 'text'}\n",
    "        ).to_dict('records')\n",
    "    else:\n",
    "        print(\"Warning: 'question_id' not found. Using 'question_text' for uniqueness.\")\n",
    "        unique_questions = scale_df.drop_duplicates(subset=['question_text_cleaned'])\n",
    "        scale_definition = unique_questions[['question_text_cleaned', 'reverse_score']].rename(\n",
    "            columns={'question_text_cleaned': 'text'}\n",
    "        ).to_dict('records')\n",
    "        for i, item in enumerate(scale_definition):\n",
    "            item['question_id'] = f\"item_{i+1}\"\n",
    "\n",
    "    if not scale_definition:\n",
    "        print(f\"Warning: Scale definition for '{scale_name_filter}' is empty after processing.\")\n",
    "    return scale_definition\n",
    "\n",
    "# --- 2. Analysis Method Functions ---\n",
    "\n",
    "def run_item_analysis(sbert_model, filtered_df_for_scale, scale_definition_list):\n",
    "    \"\"\"\n",
    "    Method 1: Item-to-Item CCR Analysis (Efficient Batch using SentenceTransformer).\n",
    "    Compares each justification to its corresponding question_text from the filtered_df_for_scale.\n",
    "    \"\"\"\n",
    "    print(f\"   Running Item-to-Item Analysis...\")\n",
    "    if filtered_df_for_scale.empty:\n",
    "        print(\"     No data to analyze for item-to-item.\")\n",
    "        return [], np.nan\n",
    "\n",
    "    item_level_results = []\n",
    "\n",
    "    if 'justification' not in filtered_df_for_scale.columns or 'question_text' not in filtered_df_for_scale.columns:\n",
    "        print(\"     Error: 'justification' or 'question_text' column missing in filtered_df_for_scale.\")\n",
    "        return [], np.nan\n",
    "\n",
    "    filtered_df_for_scale['justification'] = filtered_df_for_scale['justification'].astype(str)\n",
    "    filtered_df_for_scale['question_text_original_row'] = filtered_df_for_scale['question_text'].astype(str)\n",
    "\n",
    "    texts_to_embed_questions = filtered_df_for_scale['question_text_original_row'].tolist()\n",
    "    texts_to_embed_justifications = filtered_df_for_scale['justification'].tolist()\n",
    "\n",
    "    if not texts_to_embed_questions or not texts_to_embed_justifications:\n",
    "        print(\"     No texts found for embedding in item-to-item analysis.\")\n",
    "        return [], np.nan\n",
    "\n",
    "    try:\n",
    "        question_embeddings = sbert_model.encode(texts_to_embed_questions, convert_to_tensor=True, show_progress_bar=False)\n",
    "        justification_embeddings = sbert_model.encode(texts_to_embed_justifications, convert_to_tensor=True, show_progress_bar=False)\n",
    "    except Exception as e:\n",
    "        print(f\"     Error during SBERT encoding: {e}\")\n",
    "        return [], np.nan\n",
    "\n",
    "    all_item_ccr_scores_adjusted = []\n",
    "\n",
    "    for i in range(len(texts_to_embed_questions)):\n",
    "        similarity_tensor = util.cos_sim(question_embeddings[i], justification_embeddings[i])\n",
    "        item_ccr_score = similarity_tensor.item()\n",
    "\n",
    "        is_reversed = filtered_df_for_scale.iloc[i]['reverse_score']\n",
    "        if not isinstance(is_reversed, bool):\n",
    "            is_reversed = str(is_reversed).strip().upper() == 'TRUE'\n",
    "\n",
    "        adjusted_score = -item_ccr_score if is_reversed else item_ccr_score\n",
    "        all_item_ccr_scores_adjusted.append(adjusted_score)\n",
    "\n",
    "        item_level_results.append({\n",
    "            'question_id': filtered_df_for_scale.iloc[i].get('question_id', f'q_{i}'),\n",
    "            'question_text': texts_to_embed_questions[i],\n",
    "            'justification': texts_to_embed_justifications[i],\n",
    "            'item_ccr_raw_similarity': item_ccr_score,\n",
    "            'item_ccr_adjusted_similarity': adjusted_score,\n",
    "            'item_ground_truth_score': filtered_df_for_scale.iloc[i].get('numeric_score')\n",
    "        })\n",
    "\n",
    "    overall_score = np.nanmean(all_item_ccr_scores_adjusted) if all_item_ccr_scores_adjusted else np.nan\n",
    "    print(f\"     Item-to-Item Overall CCR Score: {overall_score:.4f}\")\n",
    "    return item_level_results, overall_score\n",
    "\n",
    "\n",
    "def run_corpus_or_speech_analysis(text_to_analyze, scale_definition_list, analysis_type_name):\n",
    "    \"\"\"\n",
    "    Methods 2 & 3: Uses ccr.ccr_wrapper.\n",
    "    Compares a single body of text (corpus of justifications or a holistic speech)\n",
    "    against all items of a scale.\n",
    "    \"\"\"\n",
    "    print(f\"   Running {analysis_type_name} Analysis...\")\n",
    "    if not text_to_analyze or not scale_definition_list:\n",
    "        print(f\"     No text or insufficient scale definition items to analyze for {analysis_type_name}.\")\n",
    "        return np.nan\n",
    "\n",
    "    temp_data_file = os.path.join(TEMP_DIR, \"temp_single_text_data.csv\")\n",
    "    temp_q_file = os.path.join(TEMP_DIR, \"temp_scale_questions.csv\")\n",
    "\n",
    "    pd.DataFrame([{'text_col': text_to_analyze}]).to_csv(temp_data_file, index=False)\n",
    "\n",
    "    q_df_data = [{'item_text': item['text']} for item in scale_definition_list]\n",
    "    if not q_df_data:\n",
    "        print(f\"     Error: No items to write to questionnaire file for {analysis_type_name}.\")\n",
    "        return np.nan\n",
    "    pd.DataFrame(q_df_data).to_csv(temp_q_file, index=False)\n",
    "\n",
    "    overall_score = np.nan\n",
    "    try:\n",
    "        raw_ccr_output_df = ccr.ccr_wrapper(\n",
    "            data_file=temp_data_file,\n",
    "            data_col='text_col',\n",
    "            q_file=temp_q_file,\n",
    "            q_col='item_text'\n",
    "        )\n",
    "\n",
    "        if not raw_ccr_output_df.empty:\n",
    "            adjusted_item_scores = []\n",
    "            for i, scale_item_meta in enumerate(scale_definition_list):\n",
    "                sim_col_name = f\"sim_item_{i+1}\"\n",
    "                if sim_col_name in raw_ccr_output_df.columns:\n",
    "                    raw_similarity = raw_ccr_output_df.iloc[0][sim_col_name]\n",
    "                    is_reversed = scale_item_meta['reverse_score']\n",
    "                    if not isinstance(is_reversed, bool):\n",
    "                        is_reversed = str(is_reversed).strip().upper() == 'TRUE'\n",
    "                    adjusted_score = -raw_similarity if is_reversed else raw_similarity\n",
    "                    adjusted_item_scores.append(adjusted_score)\n",
    "                else:\n",
    "                    print(f\"     Warning: Column {sim_col_name} not found in ccr_wrapper output for {analysis_type_name}.\")\n",
    "                    adjusted_item_scores.append(np.nan)\n",
    "\n",
    "            overall_score = np.nanmean(adjusted_item_scores) if adjusted_item_scores else np.nan\n",
    "            print(f\"     {analysis_type_name} Overall CCR Score: {overall_score:.4f}\")\n",
    "        else:\n",
    "            print(f\"     Warning: ccr_wrapper returned empty DataFrame for {analysis_type_name}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"     Error during ccr_wrapper call for {analysis_type_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        if os.path.exists(temp_data_file):\n",
    "            os.remove(temp_data_file)\n",
    "        if os.path.exists(temp_q_file):\n",
    "            os.remove(temp_q_file)\n",
    "\n",
    "    return overall_score\n",
    "\n",
    "# --- 3. Main Execution Logic ---\n",
    "def main():\n",
    "    print(\"Starting CCR Modular Analysis Framework...\")\n",
    "\n",
    "    sbert_model = None\n",
    "    try:\n",
    "        print(f\"Loading SentenceTransformer model: {SENTENCE_TRANSFORMER_MODEL_NAME}...\")\n",
    "        sbert_model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL_NAME)\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SentenceTransformer model: {e}. Method 1 (Item-to-Item) will not be available.\")\n",
    "\n",
    "    all_responses_df = pd.read_csv(UNIFIED_RESPONSES_CSV_PATH)\n",
    "    if all_responses_df.empty:\n",
    "        print(f\"Exiting: {UNIFIED_RESPONSES_CSV_PATH} is empty or could not be loaded.\")\n",
    "        return\n",
    "\n",
    "    models_to_analyze_csv_names = [\"OpenAI\", \"Claude\", \"Grok\"]\n",
    "    prompt_styles_to_analyze = [\"extreme_liberal\", \"extreme_republican\", \"minimal\"]\n",
    "    scales_to_analyze = [\"RWA\", \"LWA\"]\n",
    "\n",
    "    analysis_results_list = []\n",
    "\n",
    "    for model_name_csv in models_to_analyze_csv_names:\n",
    "        model_name_key_for_gt_speech = SPEECH_MODEL_MAPPING.get(model_name_csv, model_name_csv)\n",
    "\n",
    "        for prompt_style in prompt_styles_to_analyze:\n",
    "            print(f\"\\nProcessing CSV Model: {model_name_csv} (Key: {model_name_key_for_gt_speech}), Prompt Style: {prompt_style}\")\n",
    "\n",
    "            current_run_results = {\n",
    "                'model_name_key': model_name_key_for_gt_speech,\n",
    "                'csv_model_name': model_name_csv,\n",
    "                'prompt_style': prompt_style\n",
    "            }\n",
    "\n",
    "            for scale_name in scales_to_analyze:\n",
    "                print(f\"\\n   Scale: {scale_name}\")\n",
    "\n",
    "                filtered_model_prompt_scale_data = load_and_filter_data(\n",
    "                    UNIFIED_RESPONSES_CSV_PATH,\n",
    "                    model_name_csv,\n",
    "                    prompt_style,\n",
    "                    scale_name\n",
    "                )\n",
    "                if filtered_model_prompt_scale_data.empty:\n",
    "                    print(f\"     Skipping {scale_name} for {model_name_csv}/{prompt_style} due to no filtered data.\")\n",
    "                    current_run_results[f\"{scale_name}_item_analysis_score\"] = np.nan\n",
    "                    current_run_results[f\"{scale_name}_corpus_analysis_score\"] = np.nan\n",
    "                    current_run_results[f\"{scale_name}_speech_analysis_score\"] = np.nan\n",
    "                    continue\n",
    "\n",
    "                current_scale_definition = get_scale_definition(all_responses_df, scale_name)\n",
    "                if not current_scale_definition:\n",
    "                    print(f\"     Skipping {scale_name} for {model_name_csv}/{prompt_style} due to no scale definition.\")\n",
    "                    current_run_results[f\"{scale_name}_item_analysis_score\"] = np.nan\n",
    "                    current_run_results[f\"{scale_name}_corpus_analysis_score\"] = np.nan\n",
    "                    current_run_results[f\"{scale_name}_speech_analysis_score\"] = np.nan\n",
    "                    continue\n",
    "\n",
    "                print(f\"     Scale definition for {scale_name} loaded with {len(current_scale_definition)} items.\")\n",
    "\n",
    "                # Method 1\n",
    "                if sbert_model:\n",
    "                    item_level_ccr_results, overall_item_score = run_item_analysis(\n",
    "                        sbert_model,\n",
    "                        filtered_model_prompt_scale_data,\n",
    "                        current_scale_definition\n",
    "                    )\n",
    "                    current_run_results[f\"{scale_name}_item_analysis_score\"] = overall_item_score\n",
    "\n",
    "                    if item_level_ccr_results:\n",
    "                        item_df = pd.DataFrame(item_level_ccr_results)\n",
    "                        valid_item_comp = item_df[['item_ccr_adjusted_similarity', 'item_ground_truth_score']].dropna()\n",
    "                        valid_item_comp['item_ccr_adjusted_similarity'] = pd.to_numeric(valid_item_comp['item_ccr_adjusted_similarity'], errors='coerce')\n",
    "                        valid_item_comp['item_ground_truth_score'] = pd.to_numeric(valid_item_comp['item_ground_truth_score'], errors='coerce')\n",
    "                        valid_item_comp = valid_item_comp.dropna()\n",
    "\n",
    "                        if len(valid_item_comp) >= 2 and \\\n",
    "                        valid_item_comp['item_ccr_adjusted_similarity'].nunique() > 1 and \\\n",
    "                        valid_item_comp['item_ground_truth_score'].nunique() > 1:\n",
    "                            item_pearson_r, item_pearson_p_val = pearsonr(valid_item_comp['item_ccr_adjusted_similarity'], valid_item_comp['item_ground_truth_score'])\n",
    "                            item_spearman_rho, item_spearman_p_val = spearmanr(valid_item_comp['item_ccr_adjusted_similarity'], valid_item_comp['item_ground_truth_score'])\n",
    "                            print(f\"     Item-Level Pearson (CCR vs GT for '{scale_name}'): r={item_pearson_r:.4f}, p={item_pearson_p_val:.4f} (N={len(valid_item_comp)})\")\n",
    "                            print(f\"     Item-Level Spearman (CCR vs GT for '{scale_name}'): rho={item_spearman_rho:.4f}, p={item_spearman_p_val:.4f} (N={len(valid_item_comp)})\")\n",
    "                        elif len(valid_item_comp) < 2 :\n",
    "                            print(f\"     Not enough valid item-level data points for {scale_name} correlation.\")\n",
    "                        else:\n",
    "                            print(f\"     Cannot compute item-level correlation for {scale_name} due to zero variance or insufficient N.\")\n",
    "                else:\n",
    "                    print(\"     Skipping Item-to-Item analysis as SBERT model failed to load.\")\n",
    "                    current_run_results[f\"{scale_name}_item_analysis_score\"] = np.nan\n",
    "\n",
    "                # Method 2\n",
    "                corpus_text = \" \".join(filtered_model_prompt_scale_data['justification'].astype(str).tolist())\n",
    "                if corpus_text.strip():\n",
    "                    overall_corpus_score = run_corpus_or_speech_analysis(\n",
    "                        corpus_text, current_scale_definition, \"Corpus of Justifications\"\n",
    "                    )\n",
    "                    current_run_results[f\"{scale_name}_corpus_analysis_score\"] = overall_corpus_score\n",
    "                else:\n",
    "                    print(f\"     Skipping Corpus of Justifications analysis for {scale_name} as corpus is empty.\")\n",
    "                    current_run_results[f\"{scale_name}_corpus_analysis_score\"] = np.nan\n",
    "\n",
    "                # Method 3\n",
    "                speech_text = HOLISTIC_SPEECHES.get(model_name_key_for_gt_speech, {}).get(prompt_style)\n",
    "                if speech_text:\n",
    "                    overall_speech_score = run_corpus_or_speech_analysis(\n",
    "                        speech_text, current_scale_definition, \"Holistic Speech\"\n",
    "                    )\n",
    "                    current_run_results[f\"{scale_name}_speech_analysis_score\"] = overall_speech_score\n",
    "                else:\n",
    "                    print(f\"     No speech found for {model_name_key_for_gt_speech} with prompt {prompt_style}. Skipping speech analysis.\")\n",
    "                    current_run_results[f\"{scale_name}_speech_analysis_score\"] = np.nan\n",
    "\n",
    "            analysis_results_list.append(current_run_results)\n",
    "\n",
    "    # --- 4. Final Comparison with Overall Ground Truth ---\n",
    "    if analysis_results_list:\n",
    "        results_df = pd.DataFrame(analysis_results_list)\n",
    "\n",
    "        comparison_final_df = pd.merge(results_df, overall_gt_df,\n",
    "                                    on=[\"model_name_key\", \"prompt_style\"],\n",
    "                                    how=\"left\")\n",
    "\n",
    "        print(\"\\n\\n--- Overall Scores Summary & Comparison ---\")\n",
    "        display_cols_summary = ['model_name_key', 'prompt_style']\n",
    "        for scale in scales_to_analyze:\n",
    "            display_cols_summary.extend([\n",
    "                f\"{scale}_item_analysis_score\",\n",
    "                f\"{scale}_corpus_analysis_score\",\n",
    "                f\"{scale}_speech_analysis_score\",\n",
    "                f\"{scale}_ground_truth\"\n",
    "            ])\n",
    "        display_cols_summary = [col for col in display_cols_summary if col in comparison_final_df.columns]\n",
    "        print(comparison_final_df[display_cols_summary].to_string())\n",
    "\n",
    "\n",
    "        print(\"\\n--- Correlation of Overall CCR-Derived Scores vs. Overall Ground Truth ---\")\n",
    "        for scale in scales_to_analyze:\n",
    "            for method_suffix in [\"item_analysis_score\", \"corpus_analysis_score\", \"speech_analysis_score\"]:\n",
    "                ccr_col = f\"{scale}_{method_suffix}\"\n",
    "                gt_col = f\"{scale}_ground_truth\"\n",
    "\n",
    "                if ccr_col in comparison_final_df.columns and gt_col in comparison_final_df.columns:\n",
    "                    valid_comp = comparison_final_df[[ccr_col, gt_col]].dropna()\n",
    "                    valid_comp[ccr_col] = pd.to_numeric(valid_comp[ccr_col], errors='coerce')\n",
    "                    valid_comp[gt_col] = pd.to_numeric(valid_comp[gt_col], errors='coerce')\n",
    "                    valid_comp = valid_comp.dropna()\n",
    "\n",
    "                    if len(valid_comp) >= 2 and valid_comp[ccr_col].nunique() > 1 and valid_comp[gt_col].nunique() > 1:\n",
    "                        pearson_r, pearson_p_val = pearsonr(valid_comp[ccr_col], valid_comp[gt_col])\n",
    "                        spearman_rho, spearman_p_val = spearmanr(valid_comp[ccr_col], valid_comp[gt_col])\n",
    "                        print(f\"   Pearson:  {ccr_col} vs {gt_col}: r={pearson_r:.4f}, p={pearson_p_val:.4f} (N={len(valid_comp)})\")\n",
    "                        print(f\"   Spearman: {ccr_col} vs {gt_col}: rho={spearman_rho:.4f}, p={spearman_p_val:.4f} (N={len(valid_comp)})\")\n",
    "                    elif len(valid_comp) < 2:\n",
    "                        print(f\"   Not enough data for {ccr_col} vs {gt_col} correlation (N={len(valid_comp)}).\")\n",
    "                    else:\n",
    "                        print(f\"   Cannot compute correlation for {ccr_col} vs {gt_col} due to zero variance or insufficient N (N={len(valid_comp)}).\")\n",
    "                else:\n",
    "                    print(f\"   Skipping correlation for {ccr_col} or {gt_col} - columns not found.\")\n",
    "    else:\n",
    "        print(\"No analysis results were generated.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
